{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42257d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import packages\n",
    "\n",
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd\n",
    "import fasttext\n",
    "import fasttext.util\n",
    "\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e24bb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in dataset\n",
    "FileName = \"../ProcessedData/training_data_clean2.csv\"\n",
    "\n",
    "df = pd.read_csv(FileName, encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "efa796fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                   0\n",
      "1                             0  0 01\n",
      "2                           0  good y\n",
      "3                       0 001 218 172\n",
      "4               0 001 218 172 starter\n",
      "                     ...             \n",
      "49995                     ampd mobile\n",
      "49996    ampd mobile dealer locations\n",
      "49997         ampd mobile lauren hill\n",
      "49998              ampd mobile phones\n",
      "49999                       ampdmobil\n",
      "Name: query, Length: 50000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# See the data\n",
    "print(df[\"query\"][0:50000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6db2fc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary with all characters\n",
    "symbol_dict = {}\n",
    "count = 1\n",
    "for word in df[\"query\"]:\n",
    "    if type(word) == str:\n",
    "        for letter in word:\n",
    "            if (symbol_dict.get(letter) == None):\n",
    "                symbol_dict[letter] = count\n",
    "                count += 1\n",
    "\n",
    "vocabSize = len(symbol_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "77fe6086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': 1, ' ': 2, '1': 3, 'g': 4, 'o': 5, 'd': 6, 'y': 7, '2': 8, '8': 9, '7': 10, 's': 11, 't': 12, 'a': 13, 'r': 14, 'e': 15, '9': 16, 'n': 17, 'p': 18, 'c': 19, 'i': 20, 'b': 21, 'l': 22, 'f': 23, 'w': 24, 'q': 25, 'u': 26, 'm': 27, 'h': 28, 'v': 29, 'x': 30, 'z': 31, '4': 32, '3': 33, '6': 34, '5': 35, 'j': 36, 'k': 37}\n"
     ]
    }
   ],
   "source": [
    "print(symbol_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "317a6c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create function for one hot encoding for data loader\n",
    "def oneHotEncodeList(wordList, freqList, vocabSize, fixed_size = None):\n",
    "    oneHotVector = []\n",
    "    target_vector = []\n",
    "    for index in range(len(wordList)):\n",
    "        word = wordList[index]\n",
    "        if type(word) == str:\n",
    "            number_list = []\n",
    "            count = 0\n",
    "            for letter in word:\n",
    "                count += 1\n",
    "                if (symbol_dict.get(letter) != None and (fixed_size == None or count <= fixed_size)):\n",
    "                    number_list.append(symbol_dict.get(letter))\n",
    "            if (fixed_size != None):\n",
    "                while(len(number_list) < fixed_size):\n",
    "                        number_list.append(0)\n",
    "            oneHotWord = torch.nn.functional.one_hot(torch.tensor(number_list),vocabSize +1)\n",
    "            \n",
    "            #add more samples based on the frequency of the item.\n",
    "            for i in range(freqList[index]):\n",
    "                oneHotVector.append(oneHotWord)\n",
    "                target_vector.append(torch.tensor(freqList[index]).float())\n",
    "    return oneHotVector, target_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8f21de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "960b269d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dataloader\n",
    "#Dataloader with one hot encoding\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self,file_name, vocabSize, dataSize = None, fixed_size = None):\n",
    "        if (dataSize != None):\n",
    "            self.df = pd.read_csv(file_name, encoding='latin-1')\n",
    "            self.wordTensor, self.target = oneHotEncodeList(self.df[\"query\"][0:dataSize], self.df[\"frequency\"][0:dataSize], vocabSize, fixed_size =  fixed_size)\n",
    "        else:\n",
    "            self.df = pd.read_csv(file_name, encoding='latin-1')\n",
    "            self.wordTensor, self.target = oneHotEncodeList(self.df[\"query\"], self.df[\"frequency\"], vocabSize, fixed_size =  fixed_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.target)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        return (self.wordTensor[idx], self.target[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "2f29b428",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MyDataset(FileName, vocabSize, dataSize = 150000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "65c31ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(79.6529)\n"
     ]
    }
   ],
   "source": [
    "# code to check the average frequncy in the data set\n",
    "total_sum = 0\n",
    "for i in range(dataset.__len__()):\n",
    "    total_sum += dataset.__getitem__(i)[1]\n",
    "print(total_sum/dataset.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "3ba551a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "trainloader = torch.utils.data.DataLoader(dataset, \n",
    "                                          batch_size = batch_size, # Set batch size to one beacuse different length of sequences. \n",
    "                                          shuffle=True, \n",
    "                                          num_workers=0,\n",
    "                                          pin_memory=True,\n",
    "                                          drop_last=True)\n",
    "dataiter = iter(trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "fa8869df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is our neural network class. every Neural Network in pytorch extends nn.Module\n",
    "class MyLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers = 1):\n",
    "        super(MyLSTM, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.LSTM = nn.LSTM(self.input_dim, self.hidden_dim, self.num_layers)\n",
    "        self.linear1 = nn.Linear(self.hidden_dim, 1)\n",
    "        self.linear2 = nn.Linear(25, 10)\n",
    "        self.linear3 = nn.Linear(10, 1)\n",
    "        \n",
    "    #Input must be 3 dimensional (Sequence len, batch, input dimensions)\n",
    "    #hc is a tuple which contains the vectors h (hidden/feedback) and c (cell state vector)\n",
    "    def forward(self,inp, hc = None):\n",
    "        if hc == None:\n",
    "            hc = self.initHC()\n",
    "        seq_len = inp.size()[0]\n",
    "        #this gives outut for each input and also (hidden and cell state vector)\n",
    "        output, hidden = self.LSTM(inp,hc)\n",
    "        #Use fully connect layer to get a single output\n",
    "        output = torch.relu(self.linear1(output[seq_len-1]))\n",
    "        #output = torch.relu(self.linear2(output))\n",
    "        #output = torch.relu(self.linear3(output))\n",
    "        return torch.squeeze(output), hidden\n",
    "    \n",
    "    def initHC(self, batch_size = batch_size):\n",
    "        #initalise hidden state and cell state\n",
    "        h = torch.zeros((self.num_layers, batch_size , self.hidden_dim))\n",
    "        c = torch.zeros((self.num_layers ,batch_size , self.hidden_dim))\n",
    "        return (h,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "4aac2985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create network:\n",
    "hidden_dim = 100\n",
    "vocab_size = vocabSize + 1\n",
    "lstm_layers = 1\n",
    "network = MyLSTM(vocab_size, hidden_dim, lstm_layers)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "\n",
    "learning_rate = 0.005\n",
    "optimizer = optim.Adam(network.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "065d899d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating hidden state and cell state.\n",
    "h = torch.zeros(hidden_dim*lstm_layers*batch_size).view(lstm_layers, batch_size, hidden_dim)\n",
    "c = torch.zeros(hidden_dim*lstm_layers*batch_size).view(lstm_layers, batch_size, hidden_dim)\n",
    "\n",
    "hc = (h,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "d42a9687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to run thorugh a single query:\n",
    "def train(network, data_pair, hidden_state):\n",
    "    #Create optimizer\n",
    "    network.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    #Create input vector from word\n",
    "    input_vector = data_pair[0].view(data_pair[0].size()[1], batch_size, vocab_size).float()\n",
    "\n",
    "    \n",
    "    # Calculate the prediction\n",
    "    output, hc = network.forward(input_vector, None)\n",
    "    # Calculate loss and backpropagate the eroor\n",
    "    loss = criterion(output, data_pair[1])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if (np.random.randint(10000) == 0):\n",
    "        print(output)\n",
    "    return loss.item()\n",
    "    \n",
    "    \n",
    "# Function to run through multiple queries:\n",
    "def iterTrain(epochs = 2):\n",
    "    loss_list = []\n",
    "    h = torch.zeros(hidden_dim*lstm_layers*batch_size).view(lstm_layers, batch_size, hidden_dim)\n",
    "    c = torch.zeros(hidden_dim*lstm_layers*batch_size).view(lstm_layers, batch_size, hidden_dim)\n",
    "\n",
    "    hc = (h,c)\n",
    "    running_loss = 0\n",
    "    item_count = 0\n",
    "    for epoch in range(epochs):\n",
    "        for data_pair in trainloader:\n",
    "            item_count += 1\n",
    "            loss = train(network, data_pair, hc)\n",
    "            loss_list.append(loss)\n",
    "            running_loss += loss\n",
    "            if (item_count % 1000 == 0):\n",
    "                print(\"Epoch {}, after {} items is the average loss for last part is {}, the overall average loss is {}\".format(epoch, item_count, sum(loss_list)/1000, running_loss/item_count))\n",
    "                loss_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "ea437285",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, after 1000 items is the average loss for last part is 83328.00945023245, the overall average loss is 83328.00945023245\n",
      "Epoch 0, after 2000 items is the average loss for last part is 103417.8817155272, the overall average loss is 93372.94558287981\n",
      "Epoch 0, after 3000 items is the average loss for last part is 77322.49415506446, the overall average loss is 88022.79510694137\n",
      "Epoch 0, after 4000 items is the average loss for last part is 83514.55815464113, the overall average loss is 86895.73586886631\n",
      "Epoch 0, after 5000 items is the average loss for last part is 106818.74286954907, the overall average loss is 90880.33726900286\n",
      "Epoch 0, after 6000 items is the average loss for last part is 80645.30187581349, the overall average loss is 89174.49803680464\n",
      "Epoch 0, after 7000 items is the average loss for last part is 100198.91490952163, the overall average loss is 90749.41473290705\n",
      "Epoch 0, after 8000 items is the average loss for last part is 58589.05387277694, the overall average loss is 86729.3696253908\n",
      "Epoch 0, after 9000 items is the average loss for last part is 70681.65878640824, the overall average loss is 84946.29064328162\n",
      "Epoch 0, after 10000 items is the average loss for last part is 93432.77777197016, the overall average loss is 85794.93935615048\n",
      "Epoch 0, after 11000 items is the average loss for last part is 91715.2654364889, the overall average loss is 86333.15081799944\n",
      "Epoch 0, after 12000 items is the average loss for last part is 110037.39272104335, the overall average loss is 88308.50430991976\n",
      "Epoch 0, after 13000 items is the average loss for last part is 125039.46643659925, the overall average loss is 91133.96293504894\n",
      "Epoch 0, after 14000 items is the average loss for last part is 84131.70759745968, the overall average loss is 90633.80183950686\n",
      "Epoch 0, after 15000 items is the average loss for last part is 65437.09515405043, the overall average loss is 88954.02139380976\n",
      "Epoch 0, after 16000 items is the average loss for last part is 62181.40932121408, the overall average loss is 87280.73313927255\n",
      "Epoch 0, after 17000 items is the average loss for last part is 103797.89160866234, the overall average loss is 88252.33069629545\n",
      "Epoch 0, after 18000 items is the average loss for last part is 105426.55242009036, the overall average loss is 89206.45412539518\n",
      "Epoch 0, after 19000 items is the average loss for last part is 98944.8465145818, the overall average loss is 89719.00109324712\n",
      "Epoch 0, after 20000 items is the average loss for last part is 70525.3362702095, the overall average loss is 88759.31785209522\n",
      "Epoch 0, after 21000 items is the average loss for last part is 97970.73822068457, the overall average loss is 89197.95691726616\n",
      "Epoch 0, after 22000 items is the average loss for last part is 75489.42591251362, the overall average loss is 88574.84187159558\n",
      "tensor(70.6505, grad_fn=<SqueezeBackward0>)\n",
      "Epoch 0, after 23000 items is the average loss for last part is 83980.4528617797, the overall average loss is 88375.08582769053\n",
      "Epoch 0, after 24000 items is the average loss for last part is 106703.67837418428, the overall average loss is 89138.77718379445\n",
      "tensor(81.3770, grad_fn=<SqueezeBackward0>)\n",
      "Epoch 0, after 25000 items is the average loss for last part is 91507.40043599893, the overall average loss is 89233.52211388263\n",
      "Epoch 0, after 26000 items is the average loss for last part is 85693.731646512, the overall average loss is 89097.37632667605\n",
      "Epoch 0, after 27000 items is the average loss for last part is 97021.54408880815, the overall average loss is 89390.86402156981\n",
      "Epoch 0, after 28000 items is the average loss for last part is 121656.12535652533, the overall average loss is 90543.1947835325\n",
      "Epoch 0, after 29000 items is the average loss for last part is 100410.98878766525, the overall average loss is 90883.46354229572\n",
      "Epoch 0, after 30000 items is the average loss for last part is 99086.56108429172, the overall average loss is 91156.90012702892\n",
      "Epoch 0, after 31000 items is the average loss for last part is 96647.19721684378, the overall average loss is 91334.00648476489\n",
      "Epoch 0, after 32000 items is the average loss for last part is 84822.36805221446, the overall average loss is 91130.51778374772\n",
      "Epoch 0, after 33000 items is the average loss for last part is 72224.47993378932, the overall average loss is 90557.6075458702\n",
      "Epoch 0, after 34000 items is the average loss for last part is 97854.19834758116, the overall average loss is 90772.21315768521\n",
      "Epoch 0, after 35000 items is the average loss for last part is 89569.31077246882, the overall average loss is 90737.8445181076\n",
      "Epoch 0, after 36000 items is the average loss for last part is 101014.52827381194, the overall average loss is 91023.30795576604\n",
      "Epoch 0, after 37000 items is the average loss for last part is 74937.90597841045, the overall average loss is 90588.56736178345\n",
      "tensor(5.2992, grad_fn=<SqueezeBackward0>)\n",
      "tensor(53.0364, grad_fn=<SqueezeBackward0>)\n",
      "Epoch 0, after 38000 items is the average loss for last part is 62958.426557779436, the overall average loss is 89861.45839325702\n",
      "Epoch 0, after 39000 items is the average loss for last part is 70107.11773608676, the overall average loss is 89354.93683794496\n",
      "Epoch 0, after 40000 items is the average loss for last part is 97939.28255972946, the overall average loss is 89569.54548098957\n",
      "Epoch 0, after 41000 items is the average loss for last part is 74859.00078854423, the overall average loss is 89210.7517080031\n",
      "Epoch 0, after 42000 items is the average loss for last part is 104172.08633990775, the overall average loss is 89566.97396114368\n",
      "Epoch 0, after 43000 items is the average loss for last part is 88737.17949765052, the overall average loss is 89547.67641548102\n",
      "Epoch 0, after 44000 items is the average loss for last part is 63156.7171139825, the overall average loss is 88947.88188590154\n",
      "Epoch 0, after 45000 items is the average loss for last part is 68748.6888576448, the overall average loss is 88499.01092971808\n",
      "Epoch 0, after 46000 items is the average loss for last part is 61487.66412861604, the overall average loss is 87911.80773838976\n",
      "Epoch 0, after 47000 items is the average loss for last part is 94844.6269743543, the overall average loss is 88059.31453064432\n",
      "Epoch 0, after 48000 items is the average loss for last part is 76816.13048464447, the overall average loss is 87825.081529686\n",
      "Epoch 0, after 49000 items is the average loss for last part is 68753.00127869358, the overall average loss is 87435.85540211473\n",
      "Epoch 0, after 50000 items is the average loss for last part is 56554.63593311709, the overall average loss is 86818.2310127347\n",
      "Epoch 0, after 51000 items is the average loss for last part is 69927.55892666415, the overall average loss is 86487.04136398825\n",
      "Epoch 0, after 52000 items is the average loss for last part is 81174.84627730088, the overall average loss is 86384.88376616732\n",
      "Epoch 0, after 53000 items is the average loss for last part is 50830.03003050539, the overall average loss is 85714.03746926808\n",
      "Epoch 0, after 54000 items is the average loss for last part is 66644.44438832747, the overall average loss is 85360.89685665803\n",
      "Epoch 0, after 55000 items is the average loss for last part is 69806.16995039105, the overall average loss is 85078.08364018047\n",
      "Epoch 0, after 56000 items is the average loss for last part is 75070.51590483362, the overall average loss is 84899.37707347785\n",
      "Epoch 0, after 57000 items is the average loss for last part is 79647.50851325874, the overall average loss is 84807.23902856174\n",
      "Epoch 0, after 58000 items is the average loss for last part is 73811.4890646468, the overall average loss is 84617.65713263216\n",
      "Epoch 0, after 59000 items is the average loss for last part is 90013.68762386173, the overall average loss is 84709.1152765513\n",
      "Epoch 0, after 60000 items is the average loss for last part is 68714.88943320856, the overall average loss is 84442.5448458289\n",
      "Epoch 0, after 61000 items is the average loss for last part is 79997.42681985228, the overall average loss is 84369.67405851783\n",
      "Epoch 0, after 62000 items is the average loss for last part is 113396.6664777722, the overall average loss is 84837.85135560257\n",
      "Epoch 0, after 63000 items is the average loss for last part is 82550.25534591987, the overall average loss is 84801.54030782981\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, after 64000 items is the average loss for last part is 75431.92250464998, the overall average loss is 84655.1400296551\n",
      "Epoch 0, after 65000 items is the average loss for last part is 114753.47689888273, the overall average loss is 85118.19136610474\n",
      "Epoch 0, after 66000 items is the average loss for last part is 57702.9402113646, the overall average loss is 84702.80877285107\n",
      "Epoch 0, after 67000 items is the average loss for last part is 84706.68549416908, the overall average loss is 84702.86663436328\n",
      "Epoch 0, after 68000 items is the average loss for last part is 58178.67373241992, the overall average loss is 84312.80497404055\n",
      "Epoch 0, after 69000 items is the average loss for last part is 67609.03572085542, the overall average loss is 84070.72136167558\n",
      "Epoch 0, after 70000 items is the average loss for last part is 65850.33786660098, the overall average loss is 83810.4301688888\n",
      "Epoch 0, after 71000 items is the average loss for last part is 89402.53453613537, the overall average loss is 83889.19220223032\n",
      "Epoch 0, after 72000 items is the average loss for last part is 62931.13090266302, the overall average loss is 83598.10801751411\n",
      "Epoch 0, after 73000 items is the average loss for last part is 78854.21825147214, the overall average loss is 83533.12322619846\n",
      "tensor(29.8572, grad_fn=<SqueezeBackward0>)\n",
      "Epoch 0, after 74000 items is the average loss for last part is 64320.6399715509, the overall average loss is 83273.49507410859\n",
      "Epoch 0, after 75000 items is the average loss for last part is 80406.82977522, the overall average loss is 83235.27287012342\n",
      "Epoch 0, after 76000 items is the average loss for last part is 70277.21629095015, the overall average loss is 83064.77212566059\n",
      "Epoch 0, after 77000 items is the average loss for last part is 52252.06646044403, the overall average loss is 82664.60711702137\n",
      "Epoch 0, after 78000 items is the average loss for last part is 92234.86927257362, the overall average loss is 82787.3027856823\n",
      "Epoch 0, after 79000 items is the average loss for last part is 63040.71892213064, the overall average loss is 82537.34602791583\n",
      "Epoch 0, after 80000 items is the average loss for last part is 61875.359399169974, the overall average loss is 82279.07119505653\n",
      "Epoch 0, after 81000 items is the average loss for last part is 66842.69655453113, the overall average loss is 82088.49866863032\n",
      "Epoch 0, after 82000 items is the average loss for last part is 75230.60414191661, the overall average loss is 82004.86580854842\n",
      "Epoch 0, after 83000 items is the average loss for last part is 72607.27160832235, the overall average loss is 81891.64178203965\n",
      "Epoch 0, after 84000 items is the average loss for last part is 46290.31663331718, the overall average loss is 81467.81648265009\n",
      "Epoch 0, after 85000 items is the average loss for last part is 80609.77166701602, the overall average loss is 81457.72183776025\n",
      "tensor(65.6073, grad_fn=<SqueezeBackward0>)\n",
      "Epoch 0, after 86000 items is the average loss for last part is 82691.63190696831, the overall average loss is 81472.06962926267\n",
      "Epoch 0, after 87000 items is the average loss for last part is 66419.11342336297, the overall average loss is 81299.04714413737\n",
      "Epoch 0, after 88000 items is the average loss for last part is 59859.37672341028, the overall average loss is 81055.41452572003\n",
      "Epoch 0, after 89000 items is the average loss for last part is 93590.44842601374, the overall average loss is 81196.2576032514\n",
      "Epoch 0, after 90000 items is the average loss for last part is 41851.186090847674, the overall average loss is 80759.09014200249\n",
      "Epoch 0, after 91000 items is the average loss for last part is 68831.4487361083, the overall average loss is 80628.01715952012\n",
      "Epoch 0, after 92000 items is the average loss for last part is 60423.91417653259, the overall average loss is 80408.40734448767\n",
      "Epoch 0, after 93000 items is the average loss for last part is 58078.93736280428, the overall average loss is 80168.30551672765\n",
      "Epoch 0, after 94000 items is the average loss for last part is 55113.29876020243, the overall average loss is 79901.76289165822\n",
      "Epoch 0, after 95000 items is the average loss for last part is 58813.14463720949, the overall average loss is 79679.77743634822\n",
      "Epoch 0, after 96000 items is the average loss for last part is 70314.65174169773, the overall average loss is 79582.22404369563\n",
      "Epoch 0, after 97000 items is the average loss for last part is 66420.8850382269, the overall average loss is 79446.54013642277\n",
      "Epoch 0, after 98000 items is the average loss for last part is 74751.24565671524, the overall average loss is 79398.62896826252\n",
      "Epoch 0, after 99000 items is the average loss for last part is 45151.986200195424, the overall average loss is 79052.70328373663\n",
      "Epoch 0, after 100000 items is the average loss for last part is 91011.37268282255, the overall average loss is 79172.2899777275\n",
      "Epoch 0, after 101000 items is the average loss for last part is 28352.964324486198, the overall average loss is 78669.1283375964\n",
      "Epoch 0, after 102000 items is the average loss for last part is 51705.48620138764, the overall average loss is 78404.7789048885\n",
      "tensor(603.4323, grad_fn=<SqueezeBackward0>)\n",
      "tensor(197.0936, grad_fn=<SqueezeBackward0>)\n",
      "Epoch 0, after 103000 items is the average loss for last part is 53564.41761140914, the overall average loss is 78163.61034864109\n",
      "Epoch 0, after 104000 items is the average loss for last part is 60186.69163340822, the overall average loss is 77990.75536099463\n",
      "Epoch 0, after 105000 items is the average loss for last part is 54132.95469576526, the overall average loss is 77763.53821180198\n",
      "Epoch 0, after 106000 items is the average loss for last part is 52988.743109569485, the overall average loss is 77529.81372970543\n",
      "Epoch 0, after 107000 items is the average loss for last part is 42788.72109530042, the overall average loss is 77205.13062097268\n",
      "Epoch 0, after 108000 items is the average loss for last part is 73795.28484866723, the overall average loss is 77173.55797493283\n",
      "Epoch 0, after 109000 items is the average loss for last part is 66625.59440983368, the overall average loss is 77076.78766699614\n",
      "Epoch 0, after 110000 items is the average loss for last part is 43428.88294514107, the overall average loss is 76770.89762407017\n",
      "Epoch 0, after 111000 items is the average loss for last part is 48816.20237813386, the overall average loss is 76519.05352275544\n",
      "Epoch 0, after 112000 items is the average loss for last part is 36620.54036491929, the overall average loss is 76162.8167981319\n",
      "Epoch 0, after 113000 items is the average loss for last part is 37852.98702827358, the overall average loss is 75823.79175592077\n",
      "Epoch 0, after 114000 items is the average loss for last part is 62156.96633382048, the overall average loss is 75703.90732239358\n",
      "Epoch 0, after 115000 items is the average loss for last part is 35377.78360446871, the overall average loss is 75353.24537702033\n",
      "Epoch 0, after 116000 items is the average loss for last part is 30419.016299690546, the overall average loss is 74965.88133325025\n",
      "Epoch 0, after 117000 items is the average loss for last part is 35973.82563675364, the overall average loss is 74632.61589994686\n",
      "Epoch 0, after 118000 items is the average loss for last part is 33885.26227412137, the overall average loss is 74287.29934379582\n",
      "Epoch 0, after 119000 items is the average loss for last part is 42336.1466802586, the overall average loss is 74018.80226258964\n",
      "Epoch 0, after 120000 items is the average loss for last part is 30096.172985543402, the overall average loss is 73652.78035194766\n",
      "Epoch 0, after 121000 items is the average loss for last part is 23946.488540215298, the overall average loss is 73241.98455185078\n",
      "Epoch 0, after 122000 items is the average loss for last part is 34358.72706256198, the overall average loss is 72923.26932652874\n",
      "Epoch 0, after 123000 items is the average loss for last part is 25255.38641522033, the overall average loss is 72535.72556302219\n",
      "Epoch 0, after 124000 items is the average loss for last part is 29820.805650141756, the overall average loss is 72191.25040243444\n",
      "Epoch 0, after 125000 items is the average loss for last part is 28846.896610832613, the overall average loss is 71844.49557210166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, after 126000 items is the average loss for last part is 24700.303203642965, the overall average loss is 71470.33531520904\n",
      "Epoch 0, after 127000 items is the average loss for last part is 36374.28650795515, the overall average loss is 71193.98847420701\n",
      "Epoch 0, after 128000 items is the average loss for last part is 24953.643613322976, the overall average loss is 70832.73577998133\n",
      "Epoch 0, after 129000 items is the average loss for last part is 28096.69578021939, the overall average loss is 70501.44864820025\n",
      "Epoch 0, after 130000 items is the average loss for last part is 36282.601769107554, the overall average loss is 70238.22674913028\n",
      "Epoch 0, after 131000 items is the average loss for last part is 43671.12903793612, the overall average loss is 70035.42447652573\n",
      "Epoch 0, after 132000 items is the average loss for last part is 49655.04037615587, the overall average loss is 69881.02762728055\n",
      "Epoch 0, after 133000 items is the average loss for last part is 25435.776112714455, the overall average loss is 69546.85280386277\n",
      "Epoch 0, after 134000 items is the average loss for last part is 39686.45254655379, the overall average loss is 69324.01399597242\n",
      "Epoch 0, after 135000 items is the average loss for last part is 28313.975772337675, the overall average loss is 69020.23593505658\n",
      "Epoch 0, after 136000 items is the average loss for last part is 34875.78311911926, the overall average loss is 68769.17378199818\n",
      "Epoch 0, after 137000 items is the average loss for last part is 33655.823118169596, the overall average loss is 68512.87195233519\n",
      "Epoch 0, after 138000 items is the average loss for last part is 85539.4614611901, the overall average loss is 68636.2530357327\n",
      "Epoch 0, after 139000 items is the average loss for last part is 105314.2777746088, the overall average loss is 68900.12371730736\n",
      "Epoch 0, after 140000 items is the average loss for last part is 73738.40943527418, the overall average loss is 68934.68290100712\n",
      "Epoch 0, after 141000 items is the average loss for last part is 92553.75557170468, the overall average loss is 69102.1940547\n",
      "Epoch 0, after 142000 items is the average loss for last part is 67688.63290713022, the overall average loss is 69092.23939873121\n",
      "Epoch 0, after 143000 items is the average loss for last part is 68665.21887573338, the overall average loss is 69089.25324122774\n",
      "Epoch 0, after 144000 items is the average loss for last part is 48667.97559058726, the overall average loss is 68947.43881309827\n",
      "Epoch 0, after 145000 items is the average loss for last part is 108026.98275895651, the overall average loss is 69216.9529092766\n",
      "Epoch 0, after 146000 items is the average loss for last part is 68320.43838043904, the overall average loss is 69210.81239880511\n",
      "Epoch 0, after 147000 items is the average loss for last part is 57020.76925840783, the overall average loss is 69127.88693526499\n",
      "Epoch 0, after 148000 items is the average loss for last part is 55852.484612827524, the overall average loss is 69038.18827092418\n",
      "Epoch 0, after 149000 items is the average loss for last part is 61255.53838463713, the overall average loss is 68985.95572135178\n",
      "Epoch 0, after 150000 items is the average loss for last part is 235911.50286422716, the overall average loss is 70098.79270230429\n",
      "Epoch 0, after 151000 items is the average loss for last part is 161607.44015468578, the overall average loss is 70704.8102351015\n",
      "Epoch 0, after 152000 items is the average loss for last part is 118678.54265221095, the overall average loss is 71020.42689574037\n",
      "Epoch 0, after 153000 items is the average loss for last part is 96369.53383732248, the overall average loss is 71186.10733326703\n",
      "Epoch 0, after 154000 items is the average loss for last part is 72736.38181321872, the overall average loss is 71196.17405066932\n",
      "Epoch 0, after 155000 items is the average loss for last part is 75548.76393710414, the overall average loss is 71224.2552757431\n",
      "Epoch 0, after 156000 items is the average loss for last part is 71203.16408354814, the overall average loss is 71224.12007579315\n",
      "Epoch 0, after 157000 items is the average loss for last part is 69925.99040954893, the overall average loss is 71215.85173396998\n",
      "Epoch 0, after 158000 items is the average loss for last part is 82303.43317674157, the overall average loss is 71286.0263000635\n",
      "Epoch 0, after 159000 items is the average loss for last part is 69295.48575883183, the overall average loss is 71273.50717716268\n",
      "Epoch 0, after 160000 items is the average loss for last part is 73767.63557881722, the overall average loss is 71289.09547967302\n",
      "Epoch 0, after 161000 items is the average loss for last part is 87215.50879775712, the overall average loss is 71388.0173015245\n",
      "Epoch 0, after 162000 items is the average loss for last part is 71540.82396596212, the overall average loss is 71388.96055253957\n",
      "Epoch 0, after 163000 items is the average loss for last part is 73978.72131800362, the overall average loss is 71404.84865539515\n",
      "Epoch 0, after 164000 items is the average loss for last part is 76700.2935114988, the overall average loss is 71437.13795329822\n",
      "Epoch 0, after 165000 items is the average loss for last part is 56157.60827100217, the overall average loss is 71344.53474310251\n",
      "tensor(0., grad_fn=<SqueezeBackward0>)\n",
      "Epoch 0, after 166000 items is the average loss for last part is 69212.35459555597, the overall average loss is 71331.69028438238\n",
      "Epoch 0, after 167000 items is the average loss for last part is 53093.40459368398, the overall average loss is 71222.47899282131\n",
      "tensor(30.4386, grad_fn=<SqueezeBackward0>)\n",
      "Epoch 0, after 168000 items is the average loss for last part is 88302.05594971089, the overall average loss is 71324.14314137422\n",
      "Epoch 0, after 169000 items is the average loss for last part is 62497.84565531705, the overall average loss is 71271.91652903068\n",
      "Epoch 0, after 170000 items is the average loss for last part is 64259.88415891692, the overall average loss is 71230.66927979469\n",
      "Epoch 0, after 171000 items is the average loss for last part is 64513.17799080591, the overall average loss is 71191.3857050053\n",
      "Epoch 0, after 172000 items is the average loss for last part is 79889.9051577211, the overall average loss is 71241.95849252112\n",
      "Epoch 0, after 173000 items is the average loss for last part is 67554.52499076599, the overall average loss is 71220.64384800229\n",
      "Epoch 0, after 174000 items is the average loss for last part is 81442.39710869214, the overall average loss is 71279.38955639703\n",
      "Epoch 0, after 175000 items is the average loss for last part is 66830.50121840692, the overall average loss is 71253.96733732281\n",
      "Epoch 0, after 176000 items is the average loss for last part is 74992.20389334093, the overall average loss is 71275.20731775473\n",
      "Epoch 0, after 177000 items is the average loss for last part is 75033.9381747608, the overall average loss is 71296.44308530843\n",
      "tensor(0., grad_fn=<SqueezeBackward0>)\n",
      "Epoch 0, after 178000 items is the average loss for last part is 55828.30361378765, the overall average loss is 71209.54342535608\n",
      "Epoch 0, after 179000 items is the average loss for last part is 84044.18079400333, the overall average loss is 71281.24531009712\n",
      "Epoch 0, after 180000 items is the average loss for last part is 107285.5930495429, the overall average loss is 71481.26946420515\n",
      "Epoch 0, after 181000 items is the average loss for last part is 74110.48671474495, the overall average loss is 71495.79552636284\n",
      "Epoch 0, after 182000 items is the average loss for last part is 128413.93513922766, the overall average loss is 71808.53255720275\n",
      "tensor(73.0717, grad_fn=<SqueezeBackward0>)\n",
      "Epoch 0, after 183000 items is the average loss for last part is 83547.56034336615, the overall average loss is 71872.6802500233\n",
      "Epoch 0, after 184000 items is the average loss for last part is 47897.15783802738, the overall average loss is 71742.37849778422\n",
      "Epoch 0, after 185000 items is the average loss for last part is 87283.81432896698, the overall average loss is 71826.3862590339\n",
      "Epoch 0, after 186000 items is the average loss for last part is 98459.02419322828, the overall average loss is 71969.57248448656\n",
      "tensor(0., grad_fn=<SqueezeBackward0>)\n",
      "Epoch 0, after 187000 items is the average loss for last part is 58669.920087508275, the overall average loss is 71898.45134867387\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, after 188000 items is the average loss for last part is 93504.29526791534, the overall average loss is 72013.37605037198\n",
      "Epoch 0, after 189000 items is the average loss for last part is 57824.811182713864, the overall average loss is 71938.30427858545\n",
      "Epoch 0, after 190000 items is the average loss for last part is 57816.81662392062, the overall average loss is 71863.98065935037\n",
      "Epoch 0, after 191000 items is the average loss for last part is 49041.32126584823, the overall average loss is 71744.4902960336\n",
      "Epoch 0, after 192000 items is the average loss for last part is 63018.735888728515, the overall average loss is 71699.04365849558\n",
      "Epoch 0, after 193000 items is the average loss for last part is 41980.017640319275, the overall average loss is 71545.05906772781\n",
      "Epoch 0, after 194000 items is the average loss for last part is 54818.670286631444, the overall average loss is 71458.8405688562\n",
      "Epoch 0, after 195000 items is the average loss for last part is 36873.397209903625, the overall average loss is 71281.47932086159\n",
      "Epoch 0, after 196000 items is the average loss for last part is 64267.21788361362, the overall average loss is 71245.69227271237\n",
      "Epoch 0, after 197000 items is the average loss for last part is 44177.526627984604, the overall average loss is 71108.29041664777\n",
      "Epoch 0, after 198000 items is the average loss for last part is 58949.23597020742, the overall average loss is 71046.88105075667\n",
      "Epoch 0, after 199000 items is the average loss for last part is 50599.29528378041, the overall average loss is 70944.12936348542\n",
      "Epoch 0, after 200000 items is the average loss for last part is 32606.520421812, the overall average loss is 70752.44131877706\n",
      "Epoch 0, after 201000 items is the average loss for last part is 49957.938094454956, the overall average loss is 70648.98607885504\n",
      "Epoch 0, after 202000 items is the average loss for last part is 51242.13188668487, the overall average loss is 70552.91254325023\n",
      "Epoch 0, after 203000 items is the average loss for last part is 36312.736981164475, the overall average loss is 70384.24172767343\n",
      "Epoch 0, after 204000 items is the average loss for last part is 42730.08317415843, the overall average loss is 70248.68212692092\n",
      "Epoch 0, after 205000 items is the average loss for last part is 60821.34349961555, the overall average loss is 70202.69510922674\n",
      "Epoch 0, after 206000 items is the average loss for last part is 37515.64848529232, the overall average loss is 70044.02012561541\n",
      "Epoch 0, after 207000 items is the average loss for last part is 68391.50396046683, the overall average loss is 70036.03695573547\n",
      "Epoch 0, after 208000 items is the average loss for last part is 44641.38779996418, the overall average loss is 69913.94729633273\n",
      "Epoch 0, after 209000 items is the average loss for last part is 38314.763488788, the overall average loss is 69762.755029311\n",
      "Epoch 0, after 210000 items is the average loss for last part is 39491.39812239471, the overall average loss is 69618.60571070664\n",
      "Epoch 0, after 211000 items is the average loss for last part is 59583.951116913246, the overall average loss is 69571.04810599671\n",
      "Epoch 0, after 212000 items is the average loss for last part is 65090.491949995776, the overall average loss is 69549.91340714766\n",
      "Epoch 0, after 213000 items is the average loss for last part is 45462.84770163621, the overall average loss is 69436.82859162883\n",
      "Epoch 0, after 214000 items is the average loss for last part is 53712.103106706796, the overall average loss is 69363.34856599836\n",
      "Epoch 0, after 215000 items is the average loss for last part is 56177.29937040234, the overall average loss is 69302.0181046235\n",
      "Epoch 0, after 216000 items is the average loss for last part is 69426.6974315207, the overall average loss is 69302.59532372949\n",
      "Epoch 0, after 217000 items is the average loss for last part is 60354.979953237984, the overall average loss is 69261.3620731742\n",
      "Epoch 0, after 218000 items is the average loss for last part is 58552.894603100074, the overall average loss is 69212.24066276102\n",
      "tensor(0., grad_fn=<SqueezeBackward0>)\n",
      "Epoch 0, after 219000 items is the average loss for last part is 66360.59978815775, the overall average loss is 69199.21947155283\n",
      "Epoch 0, after 220000 items is the average loss for last part is 60447.890944886596, the overall average loss is 69159.44070552253\n",
      "Epoch 0, after 221000 items is the average loss for last part is 49084.88664476221, the overall average loss is 69068.60561927475\n",
      "Epoch 0, after 222000 items is the average loss for last part is 56386.83717691562, the overall average loss is 69011.48053620108\n",
      "Epoch 0, after 223000 items is the average loss for last part is 43780.54617936652, the overall average loss is 68898.3373328072\n",
      "Epoch 0, after 224000 items is the average loss for last part is 54268.657247954405, the overall average loss is 68833.02626099982\n",
      "Epoch 0, after 225000 items is the average loss for last part is 44431.529928245334, the overall average loss is 68724.5751661876\n",
      "Epoch 0, after 226000 items is the average loss for last part is 47078.87222829852, the overall average loss is 68628.79771955978\n",
      "Epoch 0, after 227000 items is the average loss for last part is 45962.73264215623, the overall average loss is 68528.94721261086\n",
      "Epoch 0, after 228000 items is the average loss for last part is 33315.77862983574, the overall average loss is 68374.50349075659\n",
      "Epoch 0, after 229000 items is the average loss for last part is 43072.27147408859, the overall average loss is 68264.01339461392\n",
      "Epoch 0, after 230000 items is the average loss for last part is 55818.430799655354, the overall average loss is 68209.90216594019\n",
      "Epoch 0, after 231000 items is the average loss for last part is 51090.86214392395, the overall average loss is 68135.79376757648\n",
      "Epoch 0, after 232000 items is the average loss for last part is 61591.70616553946, the overall average loss is 68107.58649342977\n",
      "Epoch 0, after 233000 items is the average loss for last part is 43181.59400134078, the overall average loss is 68000.60798488003\n",
      "Epoch 0, after 234000 items is the average loss for last part is 57187.62066715839, the overall average loss is 67954.39863736839\n",
      "Epoch 0, after 235000 items is the average loss for last part is 37992.0291392857, the overall average loss is 67826.89919269571\n",
      "Epoch 0, after 236000 items is the average loss for last part is 29196.311478250445, the overall average loss is 67663.21026170226\n",
      "Epoch 0, after 237000 items is the average loss for last part is 40777.1821424127, the overall average loss is 67549.76710508081\n",
      "Epoch 0, after 238000 items is the average loss for last part is 47614.52397196549, the overall average loss is 67466.00557931147\n",
      "Epoch 0, after 239000 items is the average loss for last part is 48790.59236225072, the overall average loss is 67387.86577505598\n",
      "Epoch 0, after 240000 items is the average loss for last part is 49408.61761361091, the overall average loss is 67312.95224104996\n",
      "Epoch 0, after 241000 items is the average loss for last part is 48902.04137400502, the overall average loss is 67236.55842002488\n",
      "Epoch 0, after 242000 items is the average loss for last part is 47060.31449966034, the overall average loss is 67153.18551126306\n",
      "tensor(268.4203, grad_fn=<SqueezeBackward0>)\n",
      "Epoch 0, after 243000 items is the average loss for last part is 135984.21896771964, the overall average loss is 67436.44079297686\n",
      "Epoch 0, after 244000 items is the average loss for last part is 108560.784594217, the overall average loss is 67604.9831856049\n",
      "Epoch 0, after 245000 items is the average loss for last part is 83889.92762107177, the overall average loss is 67671.45234656597\n",
      "tensor(0., grad_fn=<SqueezeBackward0>)\n",
      "Epoch 0, after 246000 items is the average loss for last part is 87938.74187502146, the overall average loss is 67753.8397023727\n",
      "Epoch 0, after 247000 items is the average loss for last part is 67593.04333730147, the overall average loss is 67753.18870494327\n",
      "Epoch 0, after 248000 items is the average loss for last part is 94505.27614179718, the overall average loss is 67861.06002525316\n",
      "tensor(53.1581, grad_fn=<SqueezeBackward0>)\n",
      "Epoch 0, after 249000 items is the average loss for last part is 80333.83745240359, the overall average loss is 67911.15150086422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, after 250000 items is the average loss for last part is 60728.530372529596, the overall average loss is 67882.4210163509\n",
      "Epoch 0, after 251000 items is the average loss for last part is 75413.51666956634, the overall average loss is 67912.42538150314\n",
      "Epoch 0, after 252000 items is the average loss for last part is 101514.83751063535, the overall average loss is 68045.76828677747\n",
      "Epoch 0, after 253000 items is the average loss for last part is 90492.39826244114, the overall average loss is 68134.4901443888\n",
      "Epoch 0, after 254000 items is the average loss for last part is 125885.44192421001, the overall average loss is 68361.85609627786\n",
      "Epoch 0, after 255000 items is the average loss for last part is 73729.77099305725, the overall average loss is 68382.9067429319\n",
      "Epoch 0, after 256000 items is the average loss for last part is 99505.71833650112, the overall average loss is 68504.48022571928\n",
      "Epoch 0, after 257000 items is the average loss for last part is 81395.70034473378, the overall average loss is 68554.64061528743\n",
      "Epoch 0, after 258000 items is the average loss for last part is 78320.60293774414, the overall average loss is 68592.49318242873\n",
      "tensor(0., grad_fn=<SqueezeBackward0>)\n",
      "Epoch 0, after 259000 items is the average loss for last part is 79485.67802922058, the overall average loss is 68634.55181118082\n",
      "Epoch 0, after 260000 items is the average loss for last part is 105124.31656506348, the overall average loss is 68774.89706023422\n",
      "Epoch 0, after 261000 items is the average loss for last part is 96672.93265014648, the overall average loss is 68881.78608548292\n",
      "Epoch 0, after 262000 items is the average loss for last part is 115990.31381626606, the overall average loss is 69061.58962644011\n",
      "Epoch 0, after 263000 items is the average loss for last part is 88230.18923353196, the overall average loss is 69134.47403559255\n",
      "Epoch 0, after 264000 items is the average loss for last part is 52689.197724761965, the overall average loss is 69072.18132229395\n",
      "Epoch 0, after 265000 items is the average loss for last part is 82564.92862869262, the overall average loss is 69123.09734986526\n",
      "tensor(0., grad_fn=<SqueezeBackward0>)\n",
      "Epoch 0, after 266000 items is the average loss for last part is 112362.78909787368, the overall average loss is 69285.65258200064\n",
      "Epoch 0, after 267000 items is the average loss for last part is 88982.88971324905, the overall average loss is 69359.42500571317\n",
      "Epoch 0, after 268000 items is the average loss for last part is 82842.92397031594, the overall average loss is 69409.73656901394\n",
      "Epoch 0, after 269000 items is the average loss for last part is 110972.10425962161, the overall average loss is 69564.24351210169\n",
      "tensor(0., grad_fn=<SqueezeBackward0>)\n",
      "Epoch 0, after 270000 items is the average loss for last part is 77600.96428732887, the overall average loss is 69594.00914460255\n",
      "Epoch 0, after 271000 items is the average loss for last part is 70801.39912936819, the overall average loss is 69598.46445819947\n",
      "Epoch 0, after 272000 items is the average loss for last part is 104472.05966267442, the overall average loss is 69726.67620527474\n",
      "Epoch 0, after 273000 items is the average loss for last part is 107620.89240185547, the overall average loss is 69865.48285800946\n",
      "Epoch 0, after 274000 items is the average loss for last part is 103725.66732091675, the overall average loss is 69989.06017356753\n",
      "Epoch 0, after 275000 items is the average loss for last part is 88622.7437000122, the overall average loss is 70056.8190227546\n",
      "Epoch 0, after 276000 items is the average loss for last part is 74300.35196255494, the overall average loss is 70072.1941421017\n",
      "Epoch 0, after 277000 items is the average loss for last part is 78926.26872601318, the overall average loss is 70104.15831027467\n",
      "Epoch 0, after 278000 items is the average loss for last part is 79832.2119459567, the overall average loss is 70139.15130896417\n",
      "Epoch 0, after 279000 items is the average loss for last part is 101399.45061384964, the overall average loss is 70251.19539249424\n",
      "Epoch 0, after 280000 items is the average loss for last part is 113512.56715858685, the overall average loss is 70405.70029165885\n",
      "Epoch 0, after 281000 items is the average loss for last part is 83109.76807798004, the overall average loss is 70450.91049730412\n",
      "Epoch 0, after 282000 items is the average loss for last part is 94309.9601919749, the overall average loss is 70535.51705650506\n",
      "Epoch 0, after 283000 items is the average loss for last part is 72247.06410564613, the overall average loss is 70541.56492593666\n",
      "Epoch 0, after 284000 items is the average loss for last part is 104958.83075509977, the overall average loss is 70662.75248167315\n",
      "tensor(0., grad_fn=<SqueezeBackward0>)\n",
      "Epoch 0, after 285000 items is the average loss for last part is 110794.7998002994, the overall average loss is 70803.56668279112\n",
      "tensor(0., grad_fn=<SqueezeBackward0>)\n",
      "Epoch 0, after 286000 items is the average loss for last part is 134534.87902785445, the overall average loss is 71026.4034392424\n",
      "Epoch 0, after 287000 items is the average loss for last part is 82477.4729141123, the overall average loss is 71066.30263601895\n",
      "Epoch 0, after 288000 items is the average loss for last part is 117089.59581219555, the overall average loss is 71226.10573732512\n",
      "Epoch 0, after 289000 items is the average loss for last part is 84061.01987872274, the overall average loss is 71270.51720494241\n",
      "Epoch 0, after 290000 items is the average loss for last part is 105520.16569356024, the overall average loss is 71388.61944111006\n",
      "Epoch 0, after 291000 items is the average loss for last part is 106915.66054605985, the overall average loss is 71510.70549301711\n",
      "Epoch 0, after 292000 items is the average loss for last part is 98760.89747804213, the overall average loss is 71604.0280683083\n",
      "Epoch 0, after 293000 items is the average loss for last part is 128056.11545449274, the overall average loss is 71796.69730853419\n",
      "Epoch 0, after 294000 items is the average loss for last part is 114936.25117933286, the overall average loss is 71943.43048496547\n",
      "Epoch 0, after 295000 items is the average loss for last part is 91512.3023368075, the overall average loss is 72009.76564378527\n",
      "Epoch 0, after 296000 items is the average loss for last part is 108563.90628041363, the overall average loss is 72133.25936215227\n",
      "Epoch 0, after 297000 items is the average loss for last part is 63634.58147191843, the overall average loss is 72104.64428508078\n",
      "tensor(0., grad_fn=<SqueezeBackward0>)\n",
      "Epoch 0, after 298000 items is the average loss for last part is 87401.54261310094, the overall average loss is 72155.97615866474\n",
      "Epoch 0, after 299000 items is the average loss for last part is 71616.61822736004, the overall average loss is 72154.17228598482\n",
      "tensor(0., grad_fn=<SqueezeBackward0>)\n",
      "Epoch 0, after 300000 items is the average loss for last part is 104729.40294044494, the overall average loss is 72262.75638816635\n",
      "Epoch 0, after 301000 items is the average loss for last part is 114178.30905268097, the overall average loss is 72402.01071595543\n",
      "Epoch 0, after 302000 items is the average loss for last part is 87125.63258366489, the overall average loss is 72450.76443074917\n",
      "Epoch 0, after 303000 items is the average loss for last part is 98050.71990920373, the overall average loss is 72535.25273265826\n",
      "Epoch 0, after 304000 items is the average loss for last part is 82192.69625766277, the overall average loss is 72567.02063899052\n",
      "Epoch 0, after 305000 items is the average loss for last part is 53070.46248842573, the overall average loss is 72503.09749751326\n",
      "Epoch 0, after 306000 items is the average loss for last part is 76485.6228165555, the overall average loss is 72516.11228613759\n",
      "Epoch 0, after 307000 items is the average loss for last part is 111792.53079569673, the overall average loss is 72644.04850278111\n",
      "Epoch 0, after 308000 items is the average loss for last part is 68369.69044854165, the overall average loss is 72630.17071689073\n",
      "Epoch 0, after 309000 items is the average loss for last part is 70476.67657490331, the overall average loss is 72623.20148018526\n",
      "Epoch 0, after 310000 items is the average loss for last part is 85677.64848817908, the overall average loss is 72665.31259956589\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, after 311000 items is the average loss for last part is 104489.7122880559, the overall average loss is 72767.6418590144\n",
      "Epoch 0, after 312000 items is the average loss for last part is 83999.84104670334, the overall average loss is 72803.64249743649\n",
      "tensor(0., grad_fn=<SqueezeBackward0>)\n",
      "Epoch 0, after 313000 items is the average loss for last part is 120251.70065546573, the overall average loss is 72955.23373755798\n",
      "Epoch 0, after 314000 items is the average loss for last part is 72348.40252553558, the overall average loss is 72953.3011540802\n",
      "Epoch 0, after 315000 items is the average loss for last part is 85122.03167073292, the overall average loss is 72991.93204460925\n",
      "Epoch 0, after 316000 items is the average loss for last part is 85459.89929359818, the overall average loss is 73031.38763716936\n",
      "Epoch 0, after 317000 items is the average loss for last part is 88065.11466032847, the overall average loss is 73078.81264355157\n",
      "Epoch 0, after 318000 items is the average loss for last part is 118748.34416752243, the overall average loss is 73222.42752255779\n",
      "Epoch 0, after 319000 items is the average loss for last part is 90707.84060146213, the overall average loss is 73277.24072970169\n",
      "Epoch 0, after 320000 items is the average loss for last part is 86730.08739089369, the overall average loss is 73319.2808755179\n",
      "Epoch 0, after 321000 items is the average loss for last part is 69393.11879866266, the overall average loss is 73307.04984101058\n",
      "Epoch 0, after 322000 items is the average loss for last part is 95118.51845311785, the overall average loss is 73374.78732117239\n",
      "Epoch 0, after 323000 items is the average loss for last part is 89139.99404869843, the overall average loss is 73423.5960107313\n",
      "Epoch 0, after 324000 items is the average loss for last part is 102381.07736015892, the overall average loss is 73512.97095316781\n",
      "Epoch 0, after 325000 items is the average loss for last part is 79228.6580979532, the overall average loss is 73530.55768284407\n",
      "tensor(0., grad_fn=<SqueezeBackward0>)\n",
      "Epoch 0, after 326000 items is the average loss for last part is 97789.53558227634, the overall average loss is 73604.97172548037\n",
      "Epoch 0, after 327000 items is the average loss for last part is 125068.74242652654, the overall average loss is 73762.35328725727\n",
      "Epoch 0, after 328000 items is the average loss for last part is 100964.5715427968, the overall average loss is 73845.28687949978\n",
      "Epoch 0, after 329000 items is the average loss for last part is 106460.43682239509, the overall average loss is 73944.42107385506\n",
      "tensor(0., grad_fn=<SqueezeBackward0>)\n",
      "Epoch 0, after 330000 items is the average loss for last part is 99616.16210117769, the overall average loss is 74022.21422848331\n",
      "Epoch 0, after 331000 items is the average loss for last part is 105733.09905190826, the overall average loss is 74118.01750589548\n",
      "Epoch 0, after 332000 items is the average loss for last part is 80774.32813086557, the overall average loss is 74138.06663428394\n",
      "Epoch 0, after 333000 items is the average loss for last part is 63981.765082356455, the overall average loss is 74107.5672302241\n",
      "Epoch 0, after 334000 items is the average loss for last part is 56097.316907897475, the overall average loss is 74053.64432506743\n",
      "Epoch 0, after 335000 items is the average loss for last part is 66239.12357622906, the overall average loss is 74030.31739745896\n",
      "Epoch 0, after 336000 items is the average loss for last part is 77257.12404523516, the overall average loss is 74039.92098867257\n",
      "Epoch 0, after 337000 items is the average loss for last part is 116787.35045595313, the overall average loss is 74166.76796038556\n",
      "tensor(0., grad_fn=<SqueezeBackward0>)\n",
      "tensor(0., grad_fn=<SqueezeBackward0>)\n",
      "Epoch 0, after 338000 items is the average loss for last part is 103021.90057337383, the overall average loss is 74252.1381752169\n",
      "Epoch 0, after 339000 items is the average loss for last part is 83261.07354088317, the overall average loss is 74278.71320579412\n",
      "Epoch 0, after 340000 items is the average loss for last part is 51018.23522780746, the overall average loss is 74210.30003527063\n",
      "Epoch 0, after 341000 items is the average loss for last part is 87147.88912527908, the overall average loss is 74248.24017922963\n",
      "Epoch 0, after 342000 items is the average loss for last part is 100030.71047314987, the overall average loss is 74323.62751927036\n",
      "Epoch 0, after 343000 items is the average loss for last part is 94847.17190165249, the overall average loss is 74383.46292563298\n",
      "Epoch 0, after 344000 items is the average loss for last part is 59674.147619395146, the overall average loss is 74340.70328811482\n",
      "Epoch 0, after 345000 items is the average loss for last part is 72313.23762053688, the overall average loss is 74334.82657603489\n",
      "Epoch 0, after 346000 items is the average loss for last part is 98502.48203546742, the overall average loss is 74404.67529123553\n",
      "Epoch 0, after 347000 items is the average loss for last part is 104568.3061743256, the overall average loss is 74491.60218138853\n",
      "tensor(0., grad_fn=<SqueezeBackward0>)\n",
      "Epoch 0, after 348000 items is the average loss for last part is 113827.7427224719, the overall average loss is 74604.63706800081\n",
      "tensor(0., grad_fn=<SqueezeBackward0>)\n",
      "Epoch 0, after 349000 items is the average loss for last part is 111832.17002314811, the overall average loss is 74711.30621686942\n",
      "Epoch 0, after 350000 items is the average loss for last part is 107960.16866032369, the overall average loss is 74806.30296670787\n",
      "Epoch 0, after 351000 items is the average loss for last part is 104097.81093410958, the overall average loss is 74889.75455635856\n",
      "Epoch 0, after 352000 items is the average loss for last part is 92375.53654228628, the overall average loss is 74939.43007336401\n",
      "Epoch 0, after 353000 items is the average loss for last part is 91587.27784916115, the overall average loss is 74986.5911152218\n",
      "Epoch 0, after 354000 items is the average loss for last part is 65586.12698180572, the overall average loss is 74960.03613179403\n",
      "Epoch 0, after 355000 items is the average loss for last part is nan, the overall average loss is nan\n",
      "Epoch 0, after 356000 items is the average loss for last part is nan, the overall average loss is nan\n",
      "Epoch 0, after 357000 items is the average loss for last part is nan, the overall average loss is nan\n",
      "Epoch 0, after 358000 items is the average loss for last part is nan, the overall average loss is nan\n",
      "Epoch 0, after 359000 items is the average loss for last part is nan, the overall average loss is nan\n",
      "Epoch 0, after 360000 items is the average loss for last part is nan, the overall average loss is nan\n",
      "Epoch 0, after 361000 items is the average loss for last part is nan, the overall average loss is nan\n",
      "Epoch 0, after 362000 items is the average loss for last part is nan, the overall average loss is nan\n",
      "tensor(nan, grad_fn=<SqueezeBackward0>)\n",
      "tensor(nan, grad_fn=<SqueezeBackward0>)\n",
      "Epoch 0, after 363000 items is the average loss for last part is nan, the overall average loss is nan\n",
      "Epoch 0, after 364000 items is the average loss for last part is nan, the overall average loss is nan\n",
      "Epoch 0, after 365000 items is the average loss for last part is nan, the overall average loss is nan\n",
      "Epoch 0, after 366000 items is the average loss for last part is nan, the overall average loss is nan\n",
      "Epoch 0, after 367000 items is the average loss for last part is nan, the overall average loss is nan\n",
      "Epoch 0, after 368000 items is the average loss for last part is nan, the overall average loss is nan\n",
      "Epoch 0, after 369000 items is the average loss for last part is nan, the overall average loss is nan\n",
      "Epoch 0, after 370000 items is the average loss for last part is nan, the overall average loss is nan\n",
      "Epoch 0, after 371000 items is the average loss for last part is nan, the overall average loss is nan\n",
      "Epoch 0, after 372000 items is the average loss for last part is nan, the overall average loss is nan\n",
      "Epoch 0, after 373000 items is the average loss for last part is nan, the overall average loss is nan\n",
      "Epoch 0, after 374000 items is the average loss for last part is nan, the overall average loss is nan\n",
      "Epoch 0, after 375000 items is the average loss for last part is nan, the overall average loss is nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, after 376000 items is the average loss for last part is nan, the overall average loss is nan\n",
      "Epoch 0, after 377000 items is the average loss for last part is nan, the overall average loss is nan\n",
      "Epoch 0, after 378000 items is the average loss for last part is nan, the overall average loss is nan\n",
      "Epoch 0, after 379000 items is the average loss for last part is nan, the overall average loss is nan\n",
      "Epoch 0, after 380000 items is the average loss for last part is nan, the overall average loss is nan\n",
      "Epoch 0, after 381000 items is the average loss for last part is nan, the overall average loss is nan\n",
      "tensor(nan, grad_fn=<SqueezeBackward0>)\n",
      "Epoch 0, after 382000 items is the average loss for last part is nan, the overall average loss is nan\n",
      "Epoch 0, after 383000 items is the average loss for last part is nan, the overall average loss is nan\n",
      "tensor(nan, grad_fn=<SqueezeBackward0>)\n",
      "Epoch 0, after 384000 items is the average loss for last part is nan, the overall average loss is nan\n",
      "Epoch 0, after 385000 items is the average loss for last part is nan, the overall average loss is nan\n",
      "Epoch 0, after 386000 items is the average loss for last part is nan, the overall average loss is nan\n",
      "Epoch 0, after 387000 items is the average loss for last part is nan, the overall average loss is nan\n",
      "tensor(nan, grad_fn=<SqueezeBackward0>)\n",
      "Epoch 0, after 388000 items is the average loss for last part is nan, the overall average loss is nan\n",
      "Epoch 0, after 389000 items is the average loss for last part is nan, the overall average loss is nan\n",
      "Epoch 0, after 390000 items is the average loss for last part is nan, the overall average loss is nan\n",
      "Epoch 0, after 391000 items is the average loss for last part is nan, the overall average loss is nan\n",
      "Epoch 0, after 392000 items is the average loss for last part is nan, the overall average loss is nan\n",
      "Epoch 0, after 393000 items is the average loss for last part is nan, the overall average loss is nan\n",
      "Epoch 0, after 394000 items is the average loss for last part is nan, the overall average loss is nan\n",
      "Epoch 0, after 395000 items is the average loss for last part is nan, the overall average loss is nan\n",
      "tensor(nan, grad_fn=<SqueezeBackward0>)\n",
      "Epoch 0, after 396000 items is the average loss for last part is nan, the overall average loss is nan\n",
      "Epoch 0, after 397000 items is the average loss for last part is nan, the overall average loss is nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15388/3917412207.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Run the training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0miterTrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15388/2709733915.py\u001b[0m in \u001b[0;36miterTrain\u001b[1;34m(epochs)\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdata_pair\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[0mitem_count\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_pair\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m             \u001b[0mloss_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m             \u001b[0mrunning_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15388/2709733915.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(network, data_pair, hidden_state)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;31m# Calculate the prediction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_vector\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[1;31m# Calculate loss and backpropagate the eroor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_pair\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15388/1359467773.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, inp, hc)\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mseq_len\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;31m#this gives outut for each input and also (hidden and cell state vector)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mhc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m         \u001b[1;31m#Use fully connect layer to get a single output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\DL1\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\DL1\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    677\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 679\u001b[1;33m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0m\u001b[0;32m    680\u001b[0m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0;32m    681\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Run the training\n",
    "iterTrain(epochs = 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "9b1058b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testWord(word, test_network , vocabSize = 37, fixed_size = 50):\n",
    "    number_list = []\n",
    "    count = 0\n",
    "    for letter in word:\n",
    "        count += 1\n",
    "        if (symbol_dict.get(letter) != None and (fixed_size == None or count <= fixed_size)):\n",
    "            number_list.append(symbol_dict.get(letter))\n",
    "        if (fixed_size != None):\n",
    "            while(len(number_list) < fixed_size):\n",
    "                    number_list.append(0)\n",
    "                    \n",
    "    h = torch.zeros((1,1,100))\n",
    "    c = torch.zeros((1,1,100))\n",
    "\n",
    "    hc = (h,c)\n",
    "    encoded_vector = torch.nn.functional.one_hot(torch.tensor(number_list),vocabSize +1)\n",
    "    #print(encoded_vector.size())\n",
    "    return(test_network(encoded_vector.view(encoded_vector.size()[0], 1, vocabSize +1).float(),hc)[0].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "c75f5ae3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testWord(\"google\", network)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
