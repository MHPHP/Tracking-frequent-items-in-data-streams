{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b75b1e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd\n",
    "import fasttext\n",
    "import fasttext.util\n",
    "\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2868fdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in dataset\n",
    "FileName = \"../ProcessedData/training_data_clean_words.csv\"\n",
    "\n",
    "df = pd.read_csv(FileName, encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bb98aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of                                                     query  frequency\n",
      "0                                                       a      33759\n",
      "1                                                      aa        495\n",
      "2                                                     aaa        514\n",
      "3                                                    aaaa          4\n",
      "4                                                   aaaaa          2\n",
      "...                                                   ...        ...\n",
      "457645                                          zzzzzzzzz          1\n",
      "457646  zzzzzzzzzxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx...          1\n",
      "457647  zzzzzzzzzzzzzzzzzbbbbbbbbbbbbbbbbbbbbbbbbbbbbb...          1\n",
      "457648                        zzzzzzzzzzzzzzzzzzzzzzzzzzz          2\n",
      "457649  zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz...          1\n",
      "\n",
      "[457650 rows x 2 columns]>\n"
     ]
    }
   ],
   "source": [
    "print(df.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1b6e36c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "# Downloader modellen der laver ord om til vektorer\n",
    "fasttext.util.download_model('en', if_exists='ignore')\n",
    "# Læser modellen ind. Med standard størelse 300\n",
    "ft = fasttext.load_model('cc.en.300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d137385",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Laver en liste med alle vektorer lavet med fastText. Hver vektor har en længde af 300 som standard\n",
    "vector_list = []\n",
    "for i in range(len(df[\"query\"])):\n",
    "    vector_list.append(ft.get_word_vector((str(df[\"query\"][i]))))\n",
    "vector_tensor = torch.tensor(vector_list)\n",
    "#Laver en vektor for frequency\n",
    "freq_tensor = torch.tensor(df[\"frequency\"]).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824a5840",
   "metadata": {},
   "source": [
    "Load in data for validation and turn them to fast text vectors in the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ccb9091",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in dataset\n",
    "FileName1 = \"../ProcessedData/validation_data_clean_words.csv\"\n",
    "df_val = pd.read_csv(FileName1, encoding='latin-1')\n",
    "\n",
    "#Laver en liste med alle vektorer lavet med fastText. Hver vektor har en længde af 300 som standard\n",
    "vector_list_val = []\n",
    "for i in range(len(df_val[\"query\"])):\n",
    "    vector_list_val.append(ft.get_word_vector((str(df_val[\"query\"][i]))))\n",
    "vector_tensor_val = torch.tensor(vector_list_val)\n",
    "#Laver en vektor for frequency\n",
    "freq_tensor_val = torch.tensor(df_val[\"frequency\"]).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce53fe82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Laver et simple neuralt netværk til at starte med\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.inputLayer16=nn.Linear(300, 150, bias = True)\n",
    "        self.inputLayer2=nn.Linear(150, 75, bias = True)\n",
    "        self.inputLayer3=nn.Linear(75, 30, bias = True)\n",
    "        self.inputLayer4=nn.Linear(30, 1, bias = True)\n",
    "            \n",
    "    def forward(self,x):\n",
    "        x = torch.sigmoid(self.inputLayer16(x))\n",
    "        x = torch.relu(self.inputLayer2(x))\n",
    "        x = torch.relu(self.inputLayer3(x))\n",
    "        x = torch.relu(self.inputLayer4(x))\n",
    "\n",
    "        return torch.squeeze(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "382355b1-deca-408b-800a-e392b741c23e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "457650\n",
      "79223\n"
     ]
    }
   ],
   "source": [
    "print(len(df[\"query\"]))\n",
    "print(len(df_val[\"query\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e2b87a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of                                  query  frequency\n",
      "0                                    a       3099\n",
      "1                                   aa         49\n",
      "2                                  aaa         37\n",
      "3      aaaaaaaaaaaaaaaaaaaammmmmmmmmmm          1\n",
      "4                   aaaamericanexpress          5\n",
      "...                                ...        ...\n",
      "79218                           zyrtec          9\n",
      "79219                            zyvox          1\n",
      "79220                            zyxel          5\n",
      "79221                               zz          2\n",
      "79222                           zzound          1\n",
      "\n",
      "[79223 rows x 2 columns]>\n"
     ]
    }
   ],
   "source": [
    "print(df_val.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb58d023",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_of_freq_train = sum(df[\"frequency\"])\n",
    "sum_of_freq_val = sum(df_val[\"frequency\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f42196d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " 41,\n",
       " 42,\n",
       " 43,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 47,\n",
       " 48,\n",
       " 49,\n",
       " 50,\n",
       " 51,\n",
       " 52,\n",
       " 53,\n",
       " 54,\n",
       " 55,\n",
       " 56,\n",
       " 57,\n",
       " 58,\n",
       " 59,\n",
       " 60,\n",
       " 61,\n",
       " 62,\n",
       " 63,\n",
       " 64,\n",
       " 65,\n",
       " 66,\n",
       " 67,\n",
       " 68,\n",
       " 69,\n",
       " 70,\n",
       " 71,\n",
       " 72,\n",
       " 73,\n",
       " 74,\n",
       " 75,\n",
       " 76,\n",
       " 77,\n",
       " 78,\n",
       " 79,\n",
       " 80,\n",
       " 81,\n",
       " 82,\n",
       " 83,\n",
       " 84,\n",
       " 85,\n",
       " 86,\n",
       " 87,\n",
       " 88,\n",
       " 89,\n",
       " 90,\n",
       " 91,\n",
       " 92,\n",
       " 93,\n",
       " 94,\n",
       " 95,\n",
       " 96,\n",
       " 97,\n",
       " 98,\n",
       " 99,\n",
       " 100,\n",
       " 101,\n",
       " 102,\n",
       " 103,\n",
       " 104,\n",
       " 105,\n",
       " 106,\n",
       " 107,\n",
       " 108,\n",
       " 109,\n",
       " 110,\n",
       " 111,\n",
       " 112,\n",
       " 113,\n",
       " 114,\n",
       " 115,\n",
       " 116,\n",
       " 117,\n",
       " 118,\n",
       " 119,\n",
       " 120,\n",
       " 121,\n",
       " 122,\n",
       " 123,\n",
       " 124,\n",
       " 125,\n",
       " 126,\n",
       " 127,\n",
       " 128,\n",
       " 129,\n",
       " 130,\n",
       " 131,\n",
       " 132,\n",
       " 133,\n",
       " 134,\n",
       " 135,\n",
       " 136,\n",
       " 137,\n",
       " 138,\n",
       " 139,\n",
       " 140,\n",
       " 141,\n",
       " 142,\n",
       " 143,\n",
       " 144,\n",
       " 145,\n",
       " 146,\n",
       " 147,\n",
       " 148,\n",
       " 149,\n",
       " 150,\n",
       " 151,\n",
       " 152,\n",
       " 153,\n",
       " 154,\n",
       " 155,\n",
       " 156,\n",
       " 157,\n",
       " 158,\n",
       " 159,\n",
       " 160,\n",
       " 161,\n",
       " 162,\n",
       " 163,\n",
       " 164,\n",
       " 165,\n",
       " 166,\n",
       " 167,\n",
       " 168,\n",
       " 169,\n",
       " 170,\n",
       " 171,\n",
       " 172,\n",
       " 173,\n",
       " 174,\n",
       " 175,\n",
       " 176,\n",
       " 177,\n",
       " 178,\n",
       " 179,\n",
       " 180,\n",
       " 181,\n",
       " 182,\n",
       " 183,\n",
       " 184,\n",
       " 185,\n",
       " 186,\n",
       " 187,\n",
       " 188,\n",
       " 189,\n",
       " 190,\n",
       " 191,\n",
       " 192,\n",
       " 193,\n",
       " 194,\n",
       " 195,\n",
       " 196,\n",
       " 197,\n",
       " 198,\n",
       " 199,\n",
       " 200,\n",
       " 201,\n",
       " 202,\n",
       " 203,\n",
       " 204,\n",
       " 205,\n",
       " 206,\n",
       " 207,\n",
       " 208,\n",
       " 209,\n",
       " 210,\n",
       " 211,\n",
       " 212,\n",
       " 213,\n",
       " 214,\n",
       " 215,\n",
       " 216,\n",
       " 217,\n",
       " 218,\n",
       " 219,\n",
       " 220,\n",
       " 221,\n",
       " 222,\n",
       " 223,\n",
       " 224,\n",
       " 225,\n",
       " 226,\n",
       " 227,\n",
       " 228,\n",
       " 229,\n",
       " 230,\n",
       " 231,\n",
       " 232,\n",
       " 233,\n",
       " 234,\n",
       " 235,\n",
       " 236,\n",
       " 237,\n",
       " 238,\n",
       " 239,\n",
       " 240,\n",
       " 241,\n",
       " 242,\n",
       " 243,\n",
       " 244,\n",
       " 245,\n",
       " 246,\n",
       " 247,\n",
       " 248,\n",
       " 249,\n",
       " 250,\n",
       " 251,\n",
       " 252,\n",
       " 253,\n",
       " 254,\n",
       " 255,\n",
       " 256,\n",
       " 257,\n",
       " 258,\n",
       " 259,\n",
       " 260,\n",
       " 261,\n",
       " 262,\n",
       " 263,\n",
       " 264,\n",
       " 265,\n",
       " 266,\n",
       " 267,\n",
       " 268,\n",
       " 269,\n",
       " 270,\n",
       " 271,\n",
       " 272,\n",
       " 273,\n",
       " 274,\n",
       " 275,\n",
       " 276,\n",
       " 277,\n",
       " 278,\n",
       " 279,\n",
       " 280,\n",
       " 281,\n",
       " 282,\n",
       " 283,\n",
       " 284,\n",
       " 285,\n",
       " 286,\n",
       " 287,\n",
       " 288,\n",
       " 289,\n",
       " 290,\n",
       " 291,\n",
       " 292,\n",
       " 293,\n",
       " 294,\n",
       " 295,\n",
       " 296,\n",
       " 297,\n",
       " 298,\n",
       " 299,\n",
       " 300,\n",
       " 301,\n",
       " 302,\n",
       " 303,\n",
       " 304,\n",
       " 305,\n",
       " 306,\n",
       " 307,\n",
       " 308,\n",
       " 309,\n",
       " 310,\n",
       " 311,\n",
       " 312,\n",
       " 313,\n",
       " 314,\n",
       " 315,\n",
       " 316,\n",
       " 317,\n",
       " 318,\n",
       " 319,\n",
       " 320,\n",
       " 321,\n",
       " 322,\n",
       " 323,\n",
       " 324,\n",
       " 325,\n",
       " 326,\n",
       " 327,\n",
       " 328,\n",
       " 329,\n",
       " 330,\n",
       " 331,\n",
       " 332,\n",
       " 333,\n",
       " 334,\n",
       " 335,\n",
       " 336,\n",
       " 337,\n",
       " 338,\n",
       " 339,\n",
       " 340,\n",
       " 341,\n",
       " 342,\n",
       " 343,\n",
       " 344,\n",
       " 345,\n",
       " 346,\n",
       " 347,\n",
       " 348,\n",
       " 349,\n",
       " 350,\n",
       " 351,\n",
       " 352,\n",
       " 353,\n",
       " 354,\n",
       " 355,\n",
       " 356,\n",
       " 357,\n",
       " 358,\n",
       " 359,\n",
       " 360,\n",
       " 361,\n",
       " 362,\n",
       " 363,\n",
       " 364,\n",
       " 365,\n",
       " 366,\n",
       " 367,\n",
       " 368,\n",
       " 369,\n",
       " 370,\n",
       " 371,\n",
       " 372,\n",
       " 373,\n",
       " 374,\n",
       " 375,\n",
       " 376,\n",
       " 377,\n",
       " 378,\n",
       " 379,\n",
       " 380,\n",
       " 381,\n",
       " 382,\n",
       " 383,\n",
       " 384,\n",
       " 385,\n",
       " 386,\n",
       " 387,\n",
       " 388,\n",
       " 389,\n",
       " 390,\n",
       " 391,\n",
       " 392,\n",
       " 393,\n",
       " 394,\n",
       " 395,\n",
       " 396,\n",
       " 397,\n",
       " 398,\n",
       " 399,\n",
       " 400,\n",
       " 401,\n",
       " 402,\n",
       " 403,\n",
       " 404,\n",
       " 405,\n",
       " 406,\n",
       " 407,\n",
       " 408,\n",
       " 409,\n",
       " 410,\n",
       " 411,\n",
       " 412,\n",
       " 413,\n",
       " 414,\n",
       " 415,\n",
       " 416,\n",
       " 417,\n",
       " 418,\n",
       " 419,\n",
       " 420,\n",
       " 421,\n",
       " 422,\n",
       " 423,\n",
       " 424,\n",
       " 425,\n",
       " 426,\n",
       " 427,\n",
       " 428,\n",
       " 429,\n",
       " 430,\n",
       " 431,\n",
       " 432,\n",
       " 433,\n",
       " 434,\n",
       " 435,\n",
       " 436,\n",
       " 437,\n",
       " 438,\n",
       " 439,\n",
       " 440,\n",
       " 441,\n",
       " 442,\n",
       " 443,\n",
       " 444,\n",
       " 445,\n",
       " 446,\n",
       " 447,\n",
       " 448,\n",
       " 449,\n",
       " 450,\n",
       " 451,\n",
       " 452,\n",
       " 453,\n",
       " 454,\n",
       " 455,\n",
       " 456,\n",
       " 457,\n",
       " 458,\n",
       " 459,\n",
       " 460,\n",
       " 461,\n",
       " 462,\n",
       " 463,\n",
       " 464,\n",
       " 465,\n",
       " 466,\n",
       " 467,\n",
       " 468,\n",
       " 469,\n",
       " 470,\n",
       " 471,\n",
       " 472,\n",
       " 473,\n",
       " 474,\n",
       " 475,\n",
       " 476,\n",
       " 477,\n",
       " 478,\n",
       " 479,\n",
       " 480,\n",
       " 481,\n",
       " 482,\n",
       " 483,\n",
       " 484,\n",
       " 485,\n",
       " 486,\n",
       " 487,\n",
       " 488,\n",
       " 489,\n",
       " 490,\n",
       " 491,\n",
       " 492,\n",
       " 493,\n",
       " 494,\n",
       " 495,\n",
       " 496,\n",
       " 497,\n",
       " 498,\n",
       " 499,\n",
       " 500,\n",
       " 501,\n",
       " 502,\n",
       " 503,\n",
       " 504,\n",
       " 505,\n",
       " 506,\n",
       " 507,\n",
       " 508,\n",
       " 509,\n",
       " 510,\n",
       " 511,\n",
       " 512,\n",
       " 513,\n",
       " 514,\n",
       " 515,\n",
       " 516,\n",
       " 517,\n",
       " 518,\n",
       " 519,\n",
       " 520,\n",
       " 521,\n",
       " 522,\n",
       " 523,\n",
       " 524,\n",
       " 525,\n",
       " 526,\n",
       " 527,\n",
       " 528,\n",
       " 529,\n",
       " 530,\n",
       " 531,\n",
       " 532,\n",
       " 533,\n",
       " 534,\n",
       " 535,\n",
       " 536,\n",
       " 537,\n",
       " 538,\n",
       " 539,\n",
       " 540,\n",
       " 541,\n",
       " 542,\n",
       " 543,\n",
       " 544,\n",
       " 545,\n",
       " 546,\n",
       " 547,\n",
       " 548,\n",
       " 549,\n",
       " 550,\n",
       " 551,\n",
       " 552,\n",
       " 553,\n",
       " 554,\n",
       " 555,\n",
       " 556,\n",
       " 557,\n",
       " 558,\n",
       " 559,\n",
       " 560,\n",
       " 561,\n",
       " 562,\n",
       " 563,\n",
       " 564,\n",
       " 565,\n",
       " 566,\n",
       " 567,\n",
       " 568,\n",
       " 569,\n",
       " 570,\n",
       " 571,\n",
       " 572,\n",
       " 573,\n",
       " 574,\n",
       " 575,\n",
       " 576,\n",
       " 577,\n",
       " 578,\n",
       " 579,\n",
       " 580,\n",
       " 581,\n",
       " 582,\n",
       " 583,\n",
       " 584,\n",
       " 585,\n",
       " 586,\n",
       " 587,\n",
       " 588,\n",
       " 589,\n",
       " 590,\n",
       " 591,\n",
       " 592,\n",
       " 593,\n",
       " 594,\n",
       " 595,\n",
       " 596,\n",
       " 597,\n",
       " 598,\n",
       " 599,\n",
       " 600,\n",
       " 601,\n",
       " 602,\n",
       " 603,\n",
       " 604,\n",
       " 605,\n",
       " 606,\n",
       " 607,\n",
       " 608,\n",
       " 609,\n",
       " 610,\n",
       " 611,\n",
       " 612,\n",
       " 613,\n",
       " 614,\n",
       " 615,\n",
       " 616,\n",
       " 617,\n",
       " 618,\n",
       " 619,\n",
       " 620,\n",
       " 621,\n",
       " 622,\n",
       " 623,\n",
       " 624,\n",
       " 625,\n",
       " 626,\n",
       " 627,\n",
       " 628,\n",
       " 629,\n",
       " 630,\n",
       " 631,\n",
       " 632,\n",
       " 633,\n",
       " 634,\n",
       " 635,\n",
       " 636,\n",
       " 637,\n",
       " 638,\n",
       " 639,\n",
       " 640,\n",
       " 641,\n",
       " 642,\n",
       " 643,\n",
       " 644,\n",
       " 645,\n",
       " 646,\n",
       " 647,\n",
       " 648,\n",
       " 649,\n",
       " 650,\n",
       " 651,\n",
       " 652,\n",
       " 653,\n",
       " 654,\n",
       " 655,\n",
       " 656,\n",
       " 657,\n",
       " 658,\n",
       " 659,\n",
       " 660,\n",
       " 661,\n",
       " 662,\n",
       " 663,\n",
       " 664,\n",
       " 665,\n",
       " 666,\n",
       " 667,\n",
       " 668,\n",
       " 669,\n",
       " 670,\n",
       " 671,\n",
       " 672,\n",
       " 673,\n",
       " 674,\n",
       " 675,\n",
       " 676,\n",
       " 677,\n",
       " 678,\n",
       " 679,\n",
       " 680,\n",
       " 681,\n",
       " 682,\n",
       " 683,\n",
       " 684,\n",
       " 685,\n",
       " 686,\n",
       " 687,\n",
       " 688,\n",
       " 689,\n",
       " 690,\n",
       " 691,\n",
       " 692,\n",
       " 693,\n",
       " 694,\n",
       " 695,\n",
       " 696,\n",
       " 697,\n",
       " 698,\n",
       " 699,\n",
       " 700,\n",
       " 701,\n",
       " 702,\n",
       " 703,\n",
       " 704,\n",
       " 705,\n",
       " 706,\n",
       " 707,\n",
       " 708,\n",
       " 709,\n",
       " 710,\n",
       " 711,\n",
       " 712,\n",
       " 713,\n",
       " 714,\n",
       " 715,\n",
       " 716,\n",
       " 717,\n",
       " 718,\n",
       " 719,\n",
       " 720,\n",
       " 721,\n",
       " 722,\n",
       " 723,\n",
       " 724,\n",
       " 725,\n",
       " 726,\n",
       " 727,\n",
       " 728,\n",
       " 729,\n",
       " 730,\n",
       " 731,\n",
       " 732,\n",
       " 733,\n",
       " 734,\n",
       " 735,\n",
       " 736,\n",
       " 737,\n",
       " 738,\n",
       " 739,\n",
       " 740,\n",
       " 741,\n",
       " 742,\n",
       " 743,\n",
       " 744,\n",
       " 745,\n",
       " 746,\n",
       " 747,\n",
       " 748,\n",
       " 749,\n",
       " 750,\n",
       " 751,\n",
       " 752,\n",
       " 753,\n",
       " 754,\n",
       " 755,\n",
       " 756,\n",
       " 757,\n",
       " 758,\n",
       " 759,\n",
       " 760,\n",
       " 761,\n",
       " 762,\n",
       " 763,\n",
       " 764,\n",
       " 765,\n",
       " 766,\n",
       " 767,\n",
       " 768,\n",
       " 769,\n",
       " 770,\n",
       " 771,\n",
       " 772,\n",
       " 773,\n",
       " 774,\n",
       " 775,\n",
       " 776,\n",
       " 777,\n",
       " 778,\n",
       " 779,\n",
       " 780,\n",
       " 781,\n",
       " 782,\n",
       " 783,\n",
       " 784,\n",
       " 785,\n",
       " 786,\n",
       " 787,\n",
       " 788,\n",
       " 789,\n",
       " 790,\n",
       " 791,\n",
       " 792,\n",
       " 793,\n",
       " 794,\n",
       " 795,\n",
       " 796,\n",
       " 797,\n",
       " 798,\n",
       " 799,\n",
       " 800,\n",
       " 801,\n",
       " 802,\n",
       " 803,\n",
       " 804,\n",
       " 805,\n",
       " 806,\n",
       " 807,\n",
       " 808,\n",
       " 809,\n",
       " 810,\n",
       " 811,\n",
       " 812,\n",
       " 813,\n",
       " 814,\n",
       " 815,\n",
       " 816,\n",
       " 817,\n",
       " 818,\n",
       " 819,\n",
       " 820,\n",
       " 821,\n",
       " 822,\n",
       " 823,\n",
       " 824,\n",
       " 825,\n",
       " 826,\n",
       " 827,\n",
       " 828,\n",
       " 829,\n",
       " 830,\n",
       " 831,\n",
       " 832,\n",
       " 833,\n",
       " 834,\n",
       " 835,\n",
       " 836,\n",
       " 837,\n",
       " 838,\n",
       " 839,\n",
       " 840,\n",
       " 841,\n",
       " 842,\n",
       " 843,\n",
       " 844,\n",
       " 845,\n",
       " 846,\n",
       " 847,\n",
       " 848,\n",
       " 849,\n",
       " 850,\n",
       " 851,\n",
       " 852,\n",
       " 853,\n",
       " 854,\n",
       " 855,\n",
       " 856,\n",
       " 857,\n",
       " 858,\n",
       " 859,\n",
       " 860,\n",
       " 861,\n",
       " 862,\n",
       " 863,\n",
       " 864,\n",
       " 865,\n",
       " 866,\n",
       " 867,\n",
       " 868,\n",
       " 869,\n",
       " 870,\n",
       " 871,\n",
       " 872,\n",
       " 873,\n",
       " 874,\n",
       " 875,\n",
       " 876,\n",
       " 877,\n",
       " 878,\n",
       " 879,\n",
       " 880,\n",
       " 881,\n",
       " 882,\n",
       " 883,\n",
       " 884,\n",
       " 885,\n",
       " 886,\n",
       " 887,\n",
       " 888,\n",
       " 889,\n",
       " 890,\n",
       " 891,\n",
       " 892,\n",
       " 893,\n",
       " 894,\n",
       " 895,\n",
       " 896,\n",
       " 897,\n",
       " 898,\n",
       " 899,\n",
       " 900,\n",
       " 901,\n",
       " 902,\n",
       " 903,\n",
       " 904,\n",
       " 905,\n",
       " 906,\n",
       " 907,\n",
       " 908,\n",
       " 909,\n",
       " 910,\n",
       " 911,\n",
       " 912,\n",
       " 913,\n",
       " 914,\n",
       " 915,\n",
       " 916,\n",
       " 917,\n",
       " 918,\n",
       " 919,\n",
       " 920,\n",
       " 921,\n",
       " 922,\n",
       " 923,\n",
       " 924,\n",
       " 925,\n",
       " 926,\n",
       " 927,\n",
       " 928,\n",
       " 929,\n",
       " 930,\n",
       " 931,\n",
       " 932,\n",
       " 933,\n",
       " 934,\n",
       " 935,\n",
       " 936,\n",
       " 937,\n",
       " 938,\n",
       " 939,\n",
       " 940,\n",
       " 941,\n",
       " 942,\n",
       " 943,\n",
       " 944,\n",
       " 945,\n",
       " 946,\n",
       " 947,\n",
       " 948,\n",
       " 949,\n",
       " 950,\n",
       " 951,\n",
       " 952,\n",
       " 953,\n",
       " 954,\n",
       " 955,\n",
       " 956,\n",
       " 957,\n",
       " 958,\n",
       " 959,\n",
       " 960,\n",
       " 961,\n",
       " 962,\n",
       " 963,\n",
       " 964,\n",
       " 965,\n",
       " 966,\n",
       " 967,\n",
       " 968,\n",
       " 969,\n",
       " 970,\n",
       " 971,\n",
       " 972,\n",
       " 973,\n",
       " 974,\n",
       " 975,\n",
       " 976,\n",
       " 977,\n",
       " 978,\n",
       " 979,\n",
       " 980,\n",
       " 981,\n",
       " 982,\n",
       " 983,\n",
       " 984,\n",
       " 985,\n",
       " 986,\n",
       " 987,\n",
       " 988,\n",
       " 989,\n",
       " 990,\n",
       " 991,\n",
       " 992,\n",
       " 993,\n",
       " 994,\n",
       " 995,\n",
       " 996,\n",
       " 997,\n",
       " 998,\n",
       " 999,\n",
       " ...]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys = []\n",
    "for string in range(len(df_val)):\n",
    "    keys.append(string)\n",
    "keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b5a2132a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#items =  25000 # antal af data der bruges. Kan sættes lavt for at gøre udregninger hurtigere.\n",
    "testset = torch.utils.data.TensorDataset(vector_tensor, freq_tensor)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=250)\n",
    "dataiter = iter(testloader)\n",
    "\n",
    "val_set = torch.utils.data.TensorDataset(vector_tensor_val, freq_tensor_val, torch.tensor(keys))\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "df0d3bda",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0491, grad_fn=<SqueezeBackward0>)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.tensor(np.random.uniform(size = 300,  low =0, high =1)).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3e847595",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bd0a5d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3d14b17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_graph = []\n",
    "val_mse_graph = []\n",
    "val_avg_graph = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c37cef1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3b6072a7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count 1 is: 1831\n",
      "Epoch: 0 \tTraining Loss: 10.054217\n",
      "[0, 4013, 5688, 10222, 11026, 13903, 26121, 26629, 27020, 32467, 32934, 33163, 34317, 35352, 42514, 43679, 44080, 46784, 46943, 48435, 48548, 49575, 50174, 50304, 50561, 51291, 57566, 60254, 60459, 62273, 65838, 69060, 70243, 72824, 73862, 78660]\n",
      "For the validation teh losses are MSE: 3363.070841990811 and normal loss: 6.563104776553927 \n",
      "Count 1 is: 1831\n",
      "Epoch: 1 \tTraining Loss: 9.973400\n",
      "[0, 4482, 5688, 10222, 11026, 13903, 26121, 26629, 27020, 32467, 33163, 34317, 35352, 42514, 44080, 46784, 46943, 48435, 48548, 49575, 49756, 50174, 50561, 51291, 57566, 60459, 62273, 65838, 69060, 70243, 72824, 78660]\n",
      "For the validation teh losses are MSE: 3325.8908073262837 and normal loss: 6.5667685033395085 \n",
      "Count 1 is: 1831\n",
      "Epoch: 2 \tTraining Loss: 9.896539\n",
      "[0, 5688, 10222, 11026, 13903, 26121, 26629, 27020, 32467, 33163, 34317, 35352, 46784, 46943, 48435, 48548, 49575, 50174, 50561, 51291, 57566, 60254, 60459, 62273, 65838, 69060, 70243, 73862, 78660]\n",
      "For the validation teh losses are MSE: 3254.060508063163 and normal loss: 6.547392578907194 \n",
      "Count 1 is: 1831\n",
      "Epoch: 3 \tTraining Loss: 9.887832\n",
      "[0, 5688, 10222, 11026, 13903, 26121, 26629, 27020, 31933, 32467, 33163, 34317, 35352, 40736, 42514, 46784, 46943, 48435, 48548, 49575, 50174, 50561, 51291, 57566, 60254, 60459, 62273, 65838, 69060, 70243, 73862, 78660]\n",
      "For the validation teh losses are MSE: 3167.376641736798 and normal loss: 6.526121827706177 \n",
      "Count 1 is: 1831\n",
      "Epoch: 4 \tTraining Loss: 9.809684\n",
      "[0, 5688, 10222, 11026, 13903, 26121, 26629, 27020, 31933, 32467, 32562, 33163, 34317, 35352, 40736, 44080, 46784, 46943, 48435, 48548, 49575, 50174, 50304, 50561, 51291, 57566, 60459, 62273, 65838, 68920, 69060, 70243, 72824, 73862, 78660]\n",
      "For the validation teh losses are MSE: 3071.762120686119 and normal loss: 6.432611697855808 \n",
      "Count 1 is: 1831\n",
      "Epoch: 5 \tTraining Loss: 9.760090\n",
      "[0, 4482, 5688, 11026, 13903, 26121, 26629, 27020, 31933, 32467, 32562, 33163, 34317, 35352, 42514, 43679, 46784, 46943, 48435, 48548, 50174, 50561, 51291, 57566, 60254, 60459, 61238, 62273, 65838, 69060, 70243, 72674, 72824, 73862, 78660]\n",
      "For the validation teh losses are MSE: 2961.4221428591372 and normal loss: 6.456466526639198 \n",
      "Count 1 is: 1831\n",
      "Epoch: 6 \tTraining Loss: 9.703772\n",
      "[0, 5688, 10222, 11026, 13903, 26121, 26629, 27020, 31933, 32467, 33163, 34317, 35352, 42514, 44080, 46784, 46943, 48435, 48548, 49575, 49756, 50174, 50561, 51291, 57566, 60459, 62273, 65838, 69060, 70243, 72824, 73862, 78660]\n",
      "For the validation teh losses are MSE: 2976.89582755513 and normal loss: 6.3872945564402395 \n",
      "Count 1 is: 1831\n",
      "Epoch: 7 \tTraining Loss: 9.630127\n",
      "[0, 5688, 10222, 11026, 13903, 26121, 26629, 27020, 31933, 32467, 33163, 34317, 35352, 46784, 46943, 48435, 48548, 49756, 50174, 50561, 57566, 60459, 62273, 65838, 69060, 70243, 78660]\n",
      "For the validation teh losses are MSE: 2928.4688492964497 and normal loss: 6.4811146172815315 \n",
      "Count 1 is: 1831\n",
      "Epoch: 8 \tTraining Loss: 9.607516\n",
      "[0, 4482, 5688, 7264, 10222, 11026, 13903, 26121, 26629, 27020, 31933, 32467, 32562, 32934, 33163, 34317, 35352, 40736, 42514, 44080, 46784, 46943, 48435, 48548, 49575, 49756, 50174, 50304, 50561, 50628, 51291, 54907, 57566, 60254, 60459, 61238, 62273, 65838, 68920, 69060, 70243, 72824, 73862, 77000, 78660]\n",
      "For the validation teh losses are MSE: 2692.8150148181135 and normal loss: 6.3149309402760645 \n",
      "Count 1 is: 1831\n",
      "Epoch: 9 \tTraining Loss: 9.505320\n",
      "[0, 4013, 5688, 10222, 11026, 13903, 26121, 26629, 31933, 32467, 33163, 34317, 35352, 46784, 46943, 48435, 48548, 49018, 49575, 49756, 50174, 50304, 50561, 50628, 57566, 60254, 60459, 61238, 62273, 65838, 68920, 69060, 70243, 73862, 78660]\n",
      "For the validation teh losses are MSE: 2699.120994856681 and normal loss: 6.266829935910198 \n",
      "Count 1 is: 1831\n",
      "Epoch: 10 \tTraining Loss: 9.478997\n",
      "[0, 4482, 5688, 10222, 11026, 13903, 26121, 26629, 32467, 33163, 34317, 35352, 42514, 46784, 46943, 48435, 48548, 49575, 50174, 50561, 57566, 60459, 62273, 65838, 69060, 70243, 78660]\n",
      "For the validation teh losses are MSE: 2689.6168572940287 and normal loss: 6.268179457646439 \n",
      "Count 1 is: 1831\n",
      "Epoch: 11 \tTraining Loss: 9.431861\n",
      "[0, 4482, 5688, 10222, 13903, 26121, 26629, 27020, 31933, 32467, 32934, 33163, 34317, 35352, 36506, 42514, 46784, 46943, 48435, 48548, 49575, 49756, 50174, 50561, 57566, 60254, 60459, 61238, 62273, 65838, 68920, 69060, 70243, 73862, 78660]\n",
      "For the validation teh losses are MSE: 2539.193037406127 and normal loss: 6.256092535208452 \n",
      "Count 1 is: 1831\n",
      "Epoch: 12 \tTraining Loss: 9.378609\n",
      "[0, 5688, 10222, 11026, 13903, 26121, 26629, 31933, 32467, 33163, 34317, 35352, 46784, 46943, 48435, 48548, 50174, 50561, 57566, 60459, 61238, 62273, 65838, 69060, 70243, 73862, 78660]\n",
      "For the validation teh losses are MSE: 2577.734394326195 and normal loss: 6.253573854638951 \n",
      "Count 1 is: 1831\n",
      "Epoch: 13 \tTraining Loss: 9.294570\n",
      "[0, 4013, 5688, 10222, 11026, 13903, 26121, 26629, 32467, 33163, 34317, 36506, 42514, 46784, 46943, 48435, 48548, 49575, 49756, 50174, 50561, 50628, 57566, 60459, 61238, 62273, 65838, 68920, 69060, 70243, 73862, 78660]\n",
      "For the validation teh losses are MSE: 2459.6864311687577 and normal loss: 6.197918909581301 \n",
      "Count 1 is: 1831\n",
      "Epoch: 14 \tTraining Loss: 9.298390\n",
      "[0, 4013, 4482, 5688, 10222, 11026, 13903, 26121, 26629, 27020, 32467, 33163, 34317, 35352, 46784, 46943, 48435, 48548, 49575, 49756, 50174, 50304, 50561, 50628, 57566, 60254, 60459, 62273, 65838, 68920, 69060, 70243, 78660]\n",
      "For the validation teh losses are MSE: 2367.605125433263 and normal loss: 6.147279022995982 \n",
      "Count 1 is: 1831\n",
      "Epoch: 15 \tTraining Loss: 9.257460\n",
      "[0, 5688, 11026, 13903, 26121, 26629, 32467, 33163, 34317, 35352, 36506, 42514, 46784, 46943, 48435, 48548, 49575, 49756, 50174, 50561, 50628, 57566, 60459, 62273, 65838, 68920, 69060, 70243, 78660]\n",
      "For the validation teh losses are MSE: 2416.5384886919137 and normal loss: 6.157523606477851 \n",
      "Count 1 is: 1831\n",
      "Epoch: 16 \tTraining Loss: 9.150582\n",
      "[0, 4013, 4482, 5688, 10222, 11026, 13903, 26121, 26629, 27020, 31933, 32467, 33163, 34317, 35352, 42514, 46784, 46943, 48435, 48548, 49575, 49756, 50174, 50304, 50561, 50628, 54907, 57566, 60254, 60459, 61238, 62273, 65838, 68920, 69060, 70243, 72824, 78660]\n",
      "For the validation teh losses are MSE: 2251.3528423008484 and normal loss: 6.092555546986191 \n",
      "Count 1 is: 1831\n",
      "Epoch: 17 \tTraining Loss: 9.145318\n",
      "[0, 4482, 5688, 10222, 11026, 13903, 26121, 26629, 27020, 31933, 32467, 33163, 34317, 35352, 46784, 46943, 48435, 48548, 49575, 50174, 50561, 50628, 57566, 60459, 61238, 62273, 65838, 68920, 69060, 70243, 78660]\n",
      "For the validation teh losses are MSE: 2221.9592812414803 and normal loss: 6.099078218643598 \n",
      "Count 1 is: 1831\n",
      "Epoch: 18 \tTraining Loss: 9.098030\n",
      "[0, 4013, 5688, 10222, 11026, 13903, 26121, 26629, 31933, 32467, 33163, 34317, 35352, 42514, 46784, 46943, 48435, 48548, 49575, 49756, 50174, 50561, 50628, 60459, 61238, 62273, 65838, 68920, 69060, 70243, 78660]\n",
      "For the validation teh losses are MSE: 2264.953107412681 and normal loss: 6.101637293487693 \n",
      "Count 1 is: 1831\n",
      "Epoch: 19 \tTraining Loss: 9.024117\n",
      "[0, 4482, 5688, 10222, 11026, 13903, 25812, 26121, 26629, 28691, 31933, 32467, 33163, 34317, 35352, 36506, 46784, 46943, 48435, 48548, 49575, 49756, 50174, 50304, 50561, 50628, 57566, 60459, 61238, 62273, 65838, 68920, 69060, 70243, 78660]\n",
      "For the validation teh losses are MSE: 2078.909443620628 and normal loss: 6.022345911441165 \n",
      "Count 1 is: 1831\n",
      "Epoch: 20 \tTraining Loss: 8.976276\n",
      "[0, 5688, 10222, 11026, 13903, 26121, 26629, 27020, 31933, 32467, 33163, 34317, 35352, 46784, 46943, 48435, 48548, 49575, 50174, 50561, 50628, 54907, 60459, 61238, 62273, 65838, 68920, 69060, 70243, 78660]\n",
      "For the validation teh losses are MSE: 2103.071560396381 and normal loss: 6.134066457252021 \n",
      "Count 1 is: 1831\n",
      "Epoch: 21 \tTraining Loss: 8.969231\n",
      "[0, 4482, 5688, 10222, 11026, 13903, 26121, 26629, 32467, 33163, 34317, 35352, 42514, 46784, 46943, 48435, 48548, 49575, 49756, 50174, 50304, 50561, 50628, 60254, 60459, 61238, 62273, 65838, 68920, 69060, 70243, 78660]\n",
      "For the validation teh losses are MSE: 2015.2406217502871 and normal loss: 5.995103612683173 \n",
      "Count 1 is: 1831\n",
      "Epoch: 22 \tTraining Loss: 8.925237\n",
      "[0, 4013, 4482, 5688, 11026, 13903, 25812, 26121, 26629, 32467, 33163, 34317, 35352, 42514, 46784, 46943, 48435, 48548, 49575, 49756, 50174, 50304, 50561, 50628, 60459, 61238, 62273, 65838, 68920, 69060, 70243, 78660]\n",
      "For the validation teh losses are MSE: 1903.010948289456 and normal loss: 5.902535192973982 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count 1 is: 1831\n",
      "Epoch: 23 \tTraining Loss: 8.820621\n",
      "[0, 4482, 5688, 10222, 11026, 13903, 25812, 26121, 26629, 31933, 32467, 33163, 34317, 36506, 42514, 46784, 46943, 48435, 48548, 49575, 49756, 50174, 50304, 50561, 50628, 60459, 61238, 62273, 65838, 68920, 69060, 70243, 76054, 78660]\n",
      "For the validation teh losses are MSE: 1918.1634379486181 and normal loss: 6.015099831560057 \n",
      "Count 1 is: 1831\n",
      "Epoch: 24 \tTraining Loss: 8.842304\n",
      "[0, 4482, 5688, 10222, 11026, 13903, 26121, 26629, 32467, 33163, 34317, 35352, 46784, 46943, 48435, 48548, 49575, 49756, 50174, 50561, 50628, 57566, 60459, 61238, 62273, 65838, 68920, 69060, 70243, 78660]\n",
      "For the validation teh losses are MSE: 1819.3507257696206 and normal loss: 5.914533413923126 \n",
      "Count 1 is: 1831\n",
      "Epoch: 25 \tTraining Loss: 8.756621\n",
      "[0, 5688, 10222, 11026, 13903, 25812, 26121, 26629, 27020, 31933, 32467, 33163, 34317, 35352, 36506, 42514, 46784, 46943, 48435, 48548, 49575, 49756, 50174, 50304, 50561, 50628, 54907, 57566, 60459, 61238, 62273, 65838, 68920, 69060, 70243, 78660]\n",
      "For the validation teh losses are MSE: 1774.789517327438 and normal loss: 5.967092835564719 \n",
      "Count 1 is: 1831\n",
      "Epoch: 26 \tTraining Loss: 8.706066\n",
      "[0, 4482, 5688, 10222, 11026, 13903, 26121, 26629, 27020, 32467, 33163, 34317, 35352, 42514, 46784, 46943, 48435, 48548, 49575, 49756, 50174, 50561, 50628, 60459, 61238, 62273, 65838, 68920, 69060, 70243, 78660]\n",
      "For the validation teh losses are MSE: 1735.7660707389518 and normal loss: 5.949444766300334 \n",
      "Count 1 is: 1831\n",
      "Epoch: 27 \tTraining Loss: 8.654206\n",
      "[0, 5688, 11026, 13903, 26121, 26629, 31933, 32467, 33163, 34317, 35352, 46784, 46943, 48435, 48548, 49575, 50174, 50561, 57566, 60459, 61238, 62273, 65838, 68920, 69060, 70243, 72824, 78660]\n",
      "For the validation teh losses are MSE: 1706.18547160994 and normal loss: 5.978063597664096 \n",
      "Count 1 is: 1831\n",
      "Epoch: 28 \tTraining Loss: 8.617633\n",
      "[0, 4482, 5688, 10222, 11026, 13903, 16270, 25812, 26121, 26629, 27020, 31933, 32467, 33163, 34317, 35352, 42514, 46784, 46943, 48435, 48548, 49575, 49756, 50174, 50304, 50561, 50628, 60459, 61238, 62273, 65838, 68920, 69060, 70243, 72824, 76054, 78660]\n",
      "For the validation teh losses are MSE: 1583.454305519441 and normal loss: 5.804895134002252 \n",
      "Count 1 is: 1831\n",
      "Epoch: 29 \tTraining Loss: 8.539263\n",
      "[0, 5688, 10222, 11026, 13903, 26121, 26629, 31933, 32467, 33163, 34317, 35352, 42514, 46784, 46943, 48435, 48548, 49575, 50174, 50561, 50628, 57566, 60254, 60459, 61238, 62273, 65838, 68920, 69060, 70243, 72824, 78660]\n",
      "For the validation teh losses are MSE: 1631.786082872457 and normal loss: 5.8046932039952805 \n",
      "Count 1 is: 1831\n",
      "Epoch: 30 \tTraining Loss: 8.493141\n",
      "[0, 4482, 5688, 10222, 13903, 26121, 26629, 32467, 33163, 34317, 35352, 42514, 46784, 46943, 48435, 48548, 50174, 50561, 50628, 60459, 61238, 62273, 65838, 69060, 70243, 78660]\n",
      "For the validation teh losses are MSE: 1597.7288133819773 and normal loss: 5.866484161431105 \n",
      "Count 1 is: 1831\n",
      "Epoch: 31 \tTraining Loss: 8.507979\n",
      "[0, 1875, 2790, 4013, 5688, 10222, 11026, 13903, 25812, 26121, 26629, 27020, 27733, 28691, 31933, 32467, 33163, 34317, 35352, 36506, 42514, 46784, 46943, 48435, 48548, 49575, 49756, 50174, 50304, 50561, 50628, 53724, 54907, 57566, 60459, 61238, 62273, 65838, 68920, 69060, 70243, 72824, 77000, 78660, 78684]\n",
      "For the validation teh losses are MSE: 1507.7962780450046 and normal loss: 5.784661364104093 \n",
      "Count 1 is: 1831\n",
      "Epoch: 32 \tTraining Loss: 8.482148\n",
      "[0, 2790, 5688, 11026, 13903, 16270, 25812, 26121, 26629, 27020, 27733, 31933, 32467, 33163, 34317, 35352, 46784, 46943, 48435, 48548, 49575, 49756, 50174, 50304, 50561, 50628, 57566, 60459, 61238, 62273, 65838, 68920, 69060, 70243, 78660]\n",
      "For the validation teh losses are MSE: 1469.6125540146693 and normal loss: 5.725038862378815 \n",
      "Count 1 is: 1831\n",
      "Epoch: 33 \tTraining Loss: 8.431961\n",
      "[0, 5688, 10222, 13903, 26121, 26629, 27733, 32467, 33163, 34317, 35352, 42514, 46784, 46943, 48435, 48548, 49575, 49756, 50174, 50561, 60459, 61238, 62273, 65838, 68920, 69060, 70243, 78660]\n",
      "For the validation teh losses are MSE: 1604.6701764097725 and normal loss: 5.8614365042948195 \n",
      "Count 1 is: 1831\n",
      "Epoch: 34 \tTraining Loss: 8.412560\n",
      "[0, 1875, 2790, 4482, 5688, 11026, 13903, 16270, 25812, 26121, 26629, 27733, 31933, 32467, 33163, 34317, 35352, 36506, 42514, 46784, 46943, 48435, 48548, 49575, 49756, 50174, 50561, 50628, 60459, 61238, 62273, 65838, 68920, 69060, 70243, 78660, 78684]\n",
      "For the validation teh losses are MSE: 1431.5216408046638 and normal loss: 5.738910323061778 \n",
      "Count 1 is: 1831\n",
      "Epoch: 35 \tTraining Loss: 8.339005\n",
      "[0, 2790, 5688, 10222, 11026, 13903, 26121, 26629, 27733, 32467, 33163, 34317, 35352, 42514, 46784, 46943, 48435, 48548, 49575, 49756, 50174, 50561, 50628, 60459, 61238, 62273, 65838, 68920, 69060, 70243, 78660, 78684]\n",
      "For the validation teh losses are MSE: 1519.0981222958972 and normal loss: 5.791850367931161 \n",
      "Count 1 is: 1831\n",
      "Epoch: 36 \tTraining Loss: 8.363662\n",
      "[0, 2790, 5688, 11026, 13903, 16270, 25812, 26121, 26629, 27020, 27733, 31933, 32467, 33163, 34317, 35352, 36506, 42514, 46784, 46943, 48435, 48548, 49756, 50174, 50304, 50561, 50628, 57566, 60459, 61238, 62273, 65838, 68920, 69060, 70243, 78660, 78684]\n",
      "For the validation teh losses are MSE: 1394.286491628701 and normal loss: 5.674695549703171 \n",
      "Count 1 is: 1831\n",
      "Epoch: 37 \tTraining Loss: 8.289541\n",
      "[0, 2790, 4482, 5688, 10222, 11026, 13903, 16270, 25812, 26121, 26629, 27733, 28691, 32467, 33163, 34317, 35352, 36506, 42514, 46784, 46943, 48435, 48548, 49575, 50174, 50304, 50561, 50628, 53724, 57566, 60459, 61238, 62273, 65838, 68920, 69060, 70243, 77000, 78660, 78684]\n",
      "For the validation teh losses are MSE: 1442.6570967334303 and normal loss: 5.685857261016918 \n",
      "Count 1 is: 1831\n",
      "Epoch: 38 \tTraining Loss: 8.240087\n",
      "[0, 2790, 4482, 5688, 10222, 11026, 13903, 16270, 25812, 26121, 26629, 27020, 27733, 31933, 32467, 33163, 34317, 35352, 42514, 46784, 46943, 48435, 48548, 49575, 49756, 50174, 50304, 50561, 50628, 60459, 61238, 62273, 65838, 68920, 69060, 70243, 78660, 78684]\n",
      "For the validation teh losses are MSE: 1381.0448129034194 and normal loss: 5.672534798974119 \n",
      "Count 1 is: 1831\n",
      "Epoch: 39 \tTraining Loss: 8.214683\n",
      "[0, 2790, 5688, 10222, 11026, 13903, 16270, 25812, 26121, 26629, 27020, 27733, 32467, 33163, 34317, 35352, 36506, 42514, 46784, 46943, 48435, 48548, 49575, 50174, 50304, 50561, 50628, 53724, 57566, 60459, 61238, 62273, 65838, 68920, 69060, 70243, 72824, 78660]\n",
      "For the validation teh losses are MSE: 1366.843243978001 and normal loss: 5.638296317602934 \n"
     ]
    }
   ],
   "source": [
    "criterion = torch.nn.L1Loss()\n",
    "for epoch in range(epochs):\n",
    "    train_loss = 0\n",
    "    count = 0\n",
    "    for vectors, freq in testloader:\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        estimated_freq = model(vectors)\n",
    "        #print(estimated_freq)\n",
    "        #print(freq)\n",
    "        loss = criterion(estimated_freq, freq)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        count += 1\n",
    "    loss_graph.append(train_loss/count)\n",
    "    #print(estimated_freq)\n",
    "    print(f\"Count 1 is: {count}\" )\n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch, loss_graph[len(loss_graph) -1]))\n",
    "    avg_error, mse_error = testModel(model)\n",
    "    val_mse_graph.append(mse_error)\n",
    "    val_avg_graph.append(avg_error)\n",
    "    print(f\"For the validation teh losses are MSE: {val_mse_graph[len(val_mse_graph) -1]} and normal loss: {val_avg_graph[len(val_avg_graph) -1]} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e04a712c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testModel(model, delta = 0.001):\n",
    "    ## Test on validation data\n",
    "    total_error = 0\n",
    "    total_mean_sqared_error = 0\n",
    "    criterion1 = torch.nn.MSELoss()\n",
    "    criterion2 = torch.nn.L1Loss()\n",
    "    count = 0\n",
    "    treshhold = delta * sum_of_freq_val\n",
    "    predicted_heavy_list = []\n",
    "    for vectors, freq, keys in val_loader:\n",
    "        model.eval()\n",
    "        estimated_freq = model(vectors) * (sum_of_freq_val/sum_of_freq_train)\n",
    "        predicted_heavy = (estimated_freq > treshhold).numpy()\n",
    "        Mse_loss =  criterion1(estimated_freq, freq)\n",
    "        avg_loss = criterion2(estimated_freq, freq)\n",
    "        total_error += avg_loss.item()\n",
    "        total_mean_sqared_error += Mse_loss.item()\n",
    "        for x in np.argwhere(predicted_heavy):\n",
    "            predicted_heavy_list.append(keys[x].item())\n",
    "        count += 1\n",
    "    print(predicted_heavy_list)\n",
    "    return (total_error/count , total_mean_sqared_error/count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "661bbbbb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(loss_graph)\n",
    "plt.plot(val_mse_graph)\n",
    "plt.plot(val_avg_graph)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "31f2ccbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50174</th>\n",
       "      <td>of</td>\n",
       "      <td>8977</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      query  frequency\n",
       "50174    of       8977"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x =  50174\n",
    "df_val[x:x+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "347248da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 4, 5])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([1,2,3,4,5])\n",
    "a[a > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5f54a92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_word(word):\n",
    "    vector = torch.tensor(ft.get_word_vector((str(word))))\n",
    "    return(model(vector).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "70a50c87",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14797.5224609375"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_word(\"florida\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a1cd43de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to load data, and write new csv file, with predictions.\n",
    "\n",
    "FileName = \"../ProcessedData/training_data_clean_words.csv\"\n",
    "\n",
    "def get_predictions(inputfile, outputfile, model, fastTextModel, normalConstant, columnName):\n",
    "    df = pd.read_csv(inputfile, encoding='latin-1')\n",
    "    df[\"predictions\"] = \"\"\n",
    "    query_predict_dict = {}\n",
    "    for row in range(len(df)):\n",
    "        if str(df[columnName][row]) in query_predict_dict.keys():\n",
    "            df[\"predictions\"][row] = query_predict_dict[str(df[columnName][row])]\n",
    "        else:\n",
    "            fastTextVector = torch.tensor(fastTextModel.get_word_vector((str(df[columnName][row]))))\n",
    "            result = model(fastTextVector).item()/normalConstant\n",
    "            df[\"predictions\"][row] = result\n",
    "            query_predict_dict[str(df[columnName][row])] = result\n",
    "    df.to_csv(outputfile)\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c02fae37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7413439"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_of_freq = sum(df[\"frequency\"])\n",
    "sum_of_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b5506b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = get_predictions(\"../ProcessedData/experiment_data_03_25_clean_words_no_freq.csv\", \"MLPredictions_L1.csv\", model, ft, sum_of_freq, \"x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "769158f8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              x predictions\n",
      "0                        vaniqh         0.0\n",
      "1                          best    0.000408\n",
      "2                            bu         0.0\n",
      "3                       wallmar         0.0\n",
      "4                          matc         0.0\n",
      "...                         ...         ...\n",
      "750465  danielleandcassweddding         0.0\n",
      "750466         moneymakergroupc         0.0\n",
      "750467           moneymakergrou         0.0\n",
      "750468             neverstormin         0.0\n",
      "750469             neverstormin         0.0\n",
      "\n",
      "[750470 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1c486084",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         x\n",
      "0   vaniqh\n",
      "1     best\n",
      "2       bu\n",
      "3  wallmar\n",
      "4     matc\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_test = pd.read_csv(\"../ProcessedData/experiment_data_03_25_clean_words_no_freq.csv\", encoding='latin-1')\n",
    "print(df_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b53d233b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"modelSaved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5c7cd65a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235144\n"
     ]
    }
   ],
   "source": [
    "mem_params = sum([param.nelement()*param.element_size() for param in model.parameters()])\n",
    "print(mem_params)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
